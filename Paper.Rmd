---
documentclass: report
bibliography: references.bib
csl: apa.csl
lof: no
lot: no
toc: no
link-citations: yes
output:
  bookdown::pdf_document2:
    fig.caption: yes
    includes:
      in_header: [non-float-fig.tex, custom.tex]
    toc_depth: 4
geometry: left=40mm, right=20mm, top=30mm, bottom=30mm
fontsize: 12pt
papersize: A4
linestretch: 1.5
fontfamily: times
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(AER)
library(ggfortify)
library(lubridate)
library(bookdown)
library(patchwork)
library(GGally)
library(stargazer)

#title: 'Dealing with Consumer Uncertainty: Online Consumer Reviews and What makes #them helpful?'

theme_set(theme_bw(base_size = 12))
theme_update(axis.title = element_text(size = 13),
             axis.text = element_text(size = 12))

options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8,
                      fig.height = 4,
                      echo = FALSE)
```

\newpage

# Introduction {.unnumbered}

\setcounter{page}{1}

Online shopping has becoming more and more prevalent in average consumer's daily life. Electronic shopping brought many advantages to both sellers and buyers. One of the positive perks is Online Consumer Reviews (OCR). Online Consumer Reviews come in the form of electronic word-of-mouth [@dellarocas2003] and can be defined as "*peer-generated product evaluations posted on company or third party websites*" [@mudambi2010]. OCR were found to have an effect on product sales as well as consumer decision making process. [@duan2008; @lee2011; @dellarocas2007].

OCR help customers by decreasing their search costs because it is easier for the consumer to skim through many reviews on the internet than finding and asking people who actually bought the product in question. Consumers try to get information on the product they are going to buy so that they can decrease the uncertainty about that particular product. This information acquiring process (search) is costly. Furthermore, this information gathering process might be more or less costly depending on the product type, namely Search and Experience goods [@nelson1970]. Search goods, by nature, have traits that can be observed objectively before buying (e.g., GHz of a CPU of a Laptop). On the other hand, the quality of Experience goods can not be evaluated before buying and evaluations could be very subjective (e.g., One needs to watch a movie in order to understand if it is a good movie or not. Also a movie being good or bad might depend on who watched it because people have different tastes.).

However, the increased rate of electronic commerce activity also created an abundance of product reviews on the internet. The vast amount of product reviews might increase the information overload on the customer [@sun2019; @fan2021]. Unfortunately, this information overload can make the search process even costlier because it is really time consuming to evaluate thousands of consumer reviews. Besides, not all reviews have the same informativeness level [@pan2011]. Therefore, answering the question of "what makes a helpful review?" is very important because reading more and more reviews without getting useful information can only increase the search costs. On top of this, the overload of useless information might impair the decision making ability of the potential buyer, leading to a sub-optimal choice. To make the understanding of a "helpful review" more clear it can be strictly defined as "*a peer-generated product evaluation that facilitates the consumer's purchase decision process*" [@mudambi2010].

Having a good understanding of how consumers handle information and how their behaviors change with respect to information they are exposed to might even unearth newer tools to increase the social welfare because it would lead to a decreased level of information asymmetry between consumers and producers. The decreased information asymmetry might produce two benefits. First, consumers can learn to evaluate the quality of the products before purchase much more efficiently. This leaves less room for any dis-utility that can arise from purchasing a sub-par quality product. The second, the increased knowledge of the consumers towards certain products may even facilitate competition among firms. This competition may lead increased quality of the products as well as decreased prices in a given market. This is why understanding the determinants of a helpful review is important because it might reveal new insights on consumer behavior and these new insights could be used in favor of both consumers and producers.

Since OCR do not share the same level of informativeness and usefulness [@pan2011], most of the electronic retail websites implemented a very well known "helpfulness" feature to their customer review systems in order to help their customers navigate through thousands of reviews. On some websites, this feature is implemented in a way that *helpful* / *unhelpful* buttons are present and on some other websites, there is only a *helpful* button. In this study, the data was acquired from websites which support *helpful* / *unhelpful* system since it was found that helpfulness as a percentage (helpful votes over total votes) creates more powerful results than considering only helpful votes as a count variable [@ahmad2015].

The research on perceived helpfulness is still growing. In Social and Behavioral Sciences, it is common to have conflicting studies and results in the literature [@hunter1991]. In this case, the study on perceived helpfulness is not an exception. Because of this reason, more empirical studies on perceived helpfulness are required. This study aims to add one more perspective to the growing literature of customer reviews and review helpfulness by employing a unique data set as well as questioning the subject with the intuition of economics. Furthermore, this study tries to show that unstructured data (e.g., text, image, etc.) is a viable option for economic research. With unstructured data, economics may be able to explore new venues.

This study uses secondary data. The reviews were collected from two websites. Namely, IMDB and BestBuy. The data was scrapped with Selenium Webdriver for Python [@python3]. Over 400,000 reviews and their meta data were gathered initially. After the data transformation process the final sample size was over 23,000. Then this data was subjected to empirical tests. It was found out that consumers usually find extreme reviews more helpful than neutral ones. Furthermore, consumers prefer longer reviews when reviews are rated negatively. On the contrast, positive reviews with fewer words were deemed more helpful by readers on the average. In addition, the sentiment of a review also has a significant effect on the helpfulness of a review. These effects were further moderated by product types, particularly, Search and Experience goods.

The plan of this study is outlined as the following. In the next section, the literature on the subject of Review Helpfulness is going to be presented. In the second section, the related theories which would help with the understanding of the consumer behavior and review helpfulness will be laid out. Next, the data and the research methodology are going to be introduced. The results will be shared in the fourth section. In the fifth section the results are going to be discussed and then the study is going to be concluded in the last section.

\newpage

# Literature Review

The research on Online Consumer Reviews can be considered to have two main branches. One branch is heavily marketing oriented and tries to understand the effects of OCR on sales [@ghose2011; @ögüt2012]. The other branch is concerned with rather behavioral or psychological outcomes and focuses on perceived helpfulness only. The boundary between these two branches are somewhat blurry and the same independent variables were used to explain both helpfulness and sales. Furthermore, @topaloglu2019 found that review helpfulness have a moderating effect on sales. Many other papers used the perceived helpfulness to explain sales as well [@ghose2011; @lee2018]. Thus, these two branches are inter-connected with each other. Contrary to sales oriented papers which were mentioned above, this study tries to understand behavioral aspects of the review helpfulness rather than its effects on sales.

Many different independent variables and their relationship between perceived helpfulness were examined by researchers. @hong2017 groups these independent variables into two chunks. The first group is reviewer-related predictors. Variables within this group try to understand the relationship between reviewer status (reviewer expertise, number of past reviews etc.) and perceived helpfulness. [@sun2019; @ghose2011]. The second group tries to understand the review-related relationships such as review length, review extremity, discrete review emotion etc. [@mudambi2010; @ghose2011; @ren2019; @eslami2018]. @ghose2011 showed that review-related predictors have stronger prediction power than reviewer-related ones. It should be noted that these two groups are not mutually exclusive and can be employed in the same study. The choice of the variables is heavily dependent on the data itself. In this particular study, only review-related predictors were examined because the data which is available for this study only allows for review-related variables. Reviewer-related variables could not be employed because IMDB and BestBuy use different conventions to identify their users (reviewers).

Review Length (Depth) could be considered as one of the most important independent variable explaining the perceived helpfulness. It might be implemented in different ways but for the majority of the studies it is the word count in a given review. For the most of the researchers, longer reviews contain more information about the product. As the potential buyer gets more information from a single review, he decreases his search costs with minimum effort so the amount of information (words) that a review contains becomes very important. [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, other group of researchers found out that there was a threshold on review length, meaning unnecessarily long reviews might have useless information in them[@schindler2012; @huang2015]. Another group indicates the word count of the review shows diminishing returns to helpfulness, supporting the threshold idea [@baek2012; @kuan2015; @eslami2018]. Although very rare, a negative relationship between Review Length and Perceived Helpfulness was also reported [@chatterjee2020; @yang2019].

Review Rating (Review Extremity) is another important identifier of perceived helpfulness. This is usually the star rating on a 1-5 or 1-10 scale and resembles the evaluation of the reviewer for a given product. Its effect on perceived helpfulness is rather divergent in the literature. A number of researchers found out that there is a non-linear relationship between review extremity and perceived helpfulness. Specifically, a U-Shaped function which means that reviews that are on the extremes (either very negative or very positive) are more helpful than neutral reviews [@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. In other studies, linear positive relationship was found. In other words, positively rated reviews are more useful [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020]. There are also results that support negative linear relationship [@chua2014] and no effect at all [@hong2017].

Review Age is an indicator of how long the time passed after a review published on a web site. In some papers the opposite of Review Age, Review Freshness (Recency) could be employed as well. Different effects on perceived helpfulness were also observed by researchers. Some researchers discussed that a timely review is very important at conveying the true information about a certain product, making older reviews more helpful [@liu2014; @li2019; @zhou2019]. Others discussed that some web sites promote newer reviews and recent information is much more valuable than old information. Because of these more recent reviews can be considered as more helpful [@lee2014; @choi2020; @wu2021].

The counts of negative or positive words out of total word count (the proportion of negative and positive words) were employed by a handful of studies [@baek2012; @kuan2015]. These metrics can measure the sentiment of the review itself. They are usually different than Review Extremity (Review Rating). In most cases Review Extremity and Review Sentiment (Valence) do not match [@zhang2011]. @baek2012 and @kuan2015 discovered a positive relationship between the proportion of negative words and perceived helpfulness. The relationship between the proportion of positive words and perceived helpfulness was found to be negative [@kuan2015].

The classification of Experience and Search goods plays a moderating role in the literature [@mudambi2010; @sun2019; @baek2012; @lee2016]. Some of these studies showed that Review Length has a greater effect for search products than experience products [@mudambi2010; @baek2012; @lee2016]. These findings suggest that search goods can be evaluated in a more objective way than experience goods. There is also moderating effect of product type on Review Extremity. @mudambi2010 showed that moderate reviews are more helpful for experience products than search products. @baek2012 and @lee2016 had the results which indicated that more extreme reviews (negative or positive) is much more helpful for search goods than experience goods.

Many different types of research methodologies were employed in the literature of the Review Helpfulness. To my knowledge the most common empirical analysis method that was used in the literature was Tobit Regression because of the limited and the censored nature of the Reviews Helpfulness [@mudambi2010; @korfiatis2012; @huang2015; @choi2020; @ren2019; @chua2014]. Some of these studies also shared Ordinary Least Squares results alongside with the Tobit Regression results for the purpose of benchmarking [@mudambi2010; @kuan2015; @choi2020]. Ordinary Least Squares were also used as a main empirical methodology in some papers [@liu2014; @ghose2006; @lee2016]. Furthermore, Ordinary Least Squares was employed in a hierarchical manner as well [@baek2012]. Some other studies employed Multi-Level Regression model in order to deal with the potential selection bias which is present in the data [@ghose2011; @wu2021; @kuan2015]. In a rare case Ordinal Logistic Regression was one of the methods that was used to explain the determinants of the review helpfulness [@cao2011]. Predictive methods such as Neural Networks were used to understand the components of a helpful review as well [@lee2014]. One paper attacked the problem of the review helpfulness with three methods, specifically, Partial Least Squares - Structural Equation Modelling, ANOVA Analysis and Artificial Neural Networks [@eslami2018].

Any website which offer online reviews for products could be used as a data source (Yelp, Booking, BestBuy, eBay) in the literature of review helpfulness but the majority of the aforementioned studies used the data either from websites Amazon or Internet Movie Database (IMDB) because these websites offer relatively easier ways to get the needed data. They have their respective APIs (Application Programming Interface) and on top of that, it is relatively easier to scrape both websites. Additionally, both websites offer product reviews for a numerous different types of products. Amazon is a huge e-retailer and offers a wide range of items. IMDB is a home for both critic and user reviews of Movies and TV Shows. In some other studies local equivalents of Amazon were used as well. [@sun2019]. @cao2011 used data from CNET Download.com which is a software distributor.

There might be variety reasons why there are some conflicting results in the literature. First of all, many different data sources were used in the studies. Even in the same data source, for instance Amazon, there exist a wide range of products with different brands. Different sub-groups of products can receive different feedbacks. This might increase the potential noise in the data, leading conflicting results. Secondly, many different research designs and research methodologies were employed. In these different research designs different independent and control variables were used because the data at the hand may allow for different sets of variables. Lastly, there is no established theory of review helpfulness. This means that studies are rather unorganized and researchers are usually inclining towards their own domain of expertise when it comes to explaining review helpfulness.

Since there are many conflicting results in the literature of review helpfulness additional empirical contributions are important. As well as adding new empirical findings to the literature, this study tries to extend the potential predictors with the continuous interaction term between review length and review rating to test for the existence of the Negativity Bias. Furthermore, some under-utilized predictors from the literature such as the proportion of negative words and the proportion of positive words are also used to test Negativity Bias and Loss Aversion.

\newpage

# Theoretical Framework

## Search and Experience Goods Classification

Relaxing the assumption that consumers have perfect information about the products on the market raises the problem of information acquisition for consumers. Getting the complete information about products is important because economic agents want to maximize their utility by finding the best good with respect to their budget constraints on the market. The complete information is the information about both the prices and the quality levels of the goods which are available on the market. Acquiring the information about the quality of a product is harder than getting the information about its price. To explain, a consumer can check the price list of laptops and have an idea about the maximum and the minimum prices but getting the information about a laptop's technical components or a brand's technical support for its products is a lot more effort taking and costly.

In order to solve this problem @nelson1970 differentiated goods based on their search and experience attributes [@nelson1974economic; @nelson1974; @nelson1981]. Search Attributes are the qualities that can be assessed before purchase. On the contrary, Experience Attributes can only be assessed by using the product itself, in other words, evaluation is only possible after purchase. A single good can have both search and experience attributes. Depending on its dominant attributes the good can be classified as Search or Experience. Thus, search and experience classification is not usually a clear cut one, rather it could be considered as a spectrum. @darby1973 extended Search and Experience goods framework with Credence goods where consumers can not evaluate the good even after the consumption. Though, Credence goods will not be discussed any further in this study.

Consumers can choose from two different strategies when they want to acquire more information about products. Consumers "search" in order to get information about a product assuming that they perfectly know where they can obtain the knowledge. @nelson1970 defines the search process as *"The most obvious procedure available to the consumer in obtaining information about price or quality is search"*. The search process is complete when consumers inspect the option and that inspection happens before the purchase. Contrary to the search process, consumer may choose to "experience" the product. The consumer follows this strategy when the search for information is either inappropriate, inefficient or costly compared to just experiencing the good. This is true for cheap goods especially. For instance, in order to find out the best chocolate in the market it is best for the consumer to buy every other brand available and taste them. Thus, consumers will always prefer sampling or trial use (experiencing the product) given the fact that the cost of the sampling or trial is lower than the cost of the search. This is why search is costlier for Experience goods. Therefore, the kind of strategy that a consumer chooses depends on the product's Search or Experience attributes and how dominant these attributes are.

@nelson1970 also states that consumers can use other consumers' experiences in their search processes. The information about a certain good is usually acquired in the form of "word-of-mouth" with this kind of search process. This type of search is viable when getting the information about a certain product is much more costly by "experience" compared to by "search", i.e., sampling the product is very costly. For instance buying a laptop and trying to figure out if it is the best laptop for a certain task may even require buying additional laptops because of the need for comparison. This is the way of "experiencing" the good and this would be very costly. On the other hand, the potential buyer may want to hear from people who do the similar tasks with their laptops and try to sort out the best possible laptop model for her own task. This way she gets information about the product without having to "experience" it and this approach is way cheaper.

@klein1998 discusses that experience sharing becomes easier with the increasing usage of the internet. Because experiences about products can be shared much more easily on the internet, consumers' pre-purchase behavior might change accordingly. The reason of this is that internet decreases search costs considerably. Furthermore, the potential consumer may engage in "vicarious learning" where he/she gets indirect usage experience through the experiences of other consumers in the search process [@murray1991].

Ultimately, to put the theory in a more generalized way, a product's Total Cost consists of both Search Cost and Product Cost. This Search Cost is higher for Experience goods, in other words, information search is costlier for Experience goods than Search goods. In the consumer's perspective, to make the search viable, the marginal benefit of an additional search must be grater than the marginal cost of the additional search. The increasing use of internet decreases the search costs. Because of the decreased search costs, the consumer may undertake more search processes and this might change how people perceive the good types, in other words, internet and virtual experience sharing can convert some Experience goods to Search goods [@klein1998].

## Negativity Bias Theorem and Loss Aversion

Negativity Bias Theorem argues that humans or even animals have tendencies to be more aware of negative entities than positive ones. In @rozin2001's words Negativity Bias can be best exemplified such as the following:

> *"Brief contact with a cockroach will usually render a delicious meal inedible. The inverse phenomenon rendering a pile of cockroaches on a platter edible by contact with one's favorite food is unheard of. More modestly, consider a dish of a food that you are inclined to dislike: lima beans, fish, or whatever. What could you touch to that food to make it desirable to eat that is, what is the anti-cockroach? Nothing! . . . One of the best generic descriptions of this relative power of negative contamination is embedded in an age old Russian adage: 'A spoonful of tar can spoil a barrel of honey, but a spoonful of honey does nothing for a barrel of tar.'"*

@rozin2001 explains the Negativity Bias Theorem by describing it with four main aspects. Namely, Negative Potency, Steeper Negative Gradients, Negative Dominance and Negative Differentiation [@rozin2001; @sen2007; @eslami2018]. Loss Aversion has a strong connection to Negativity Bias Theorem since negative cues might be used as an early warning against potential losses [@herr1991].

**Negative Potency** defends that the exact opposite of something positive carries heavier weight. In other words, negative events are much more impactful than their exact positive counterparts. For instance, losing 300\$ could have a harder psychological effect on a person than that particular person is gaining 300\$. Loss Aversion [@kahneman1979; @tversky1991] is the phenomenon where the Negative Potency can be observed the best. People risk more (less) when they are losing (gaining) something. This behavior can be explained by the Endowment Effect [@kahneman1990]. Endowment effect occurs because people usually over-estimate the value of their belongings, for example a pencil. However, they would have under-valued the same pencil if the item did not belong to them. The reason to this behavior is that the dis-utility of losing that pencil is greater than the utility of acquiring the same pencil.

**Greater Steepness of Negative Gradients** simply implies that the marginal psychological effect of a negative entity is much more higher than a positive one. In other words, negative things cause more negativity as the time goes on. At the very least, the effects of the negativeness are a lot more long-standing than effects of the positiveness.

**Negativity Dominance** asserts that a balanced mixture of positive and negative evaluations creates a perception of negativity. This means that negative entities have higher weight than positive entities. @rozin2001 also puts forwards as an example:

> *"The image that comes to mind from the physical domain is that of a single cancerous growth or germ that radiates itself through and ultimately consumes a perfectly healthy body. The image that comes to mind from the moral domain is that of a single vice corrupting and perverting and bringing the moral downfall of an otherwise perfectly good person."*

**Greater Negative Differentiation** discusses the possibility that human cognition is fine-tuned to negative entities because being aware of negative stimuli might correlate with survival in the process of the evolution. As a consequence, the vocabulary that reflects and describes the state of the negativeness is much more wider than its counterpart the positiveness. [@peeters1971].

Prospect Theory (Loss Aversion) criticizes Expected Utility Theory by introducing the thought that Expected Utility Theory is insufficient explaining the decision making behavior of economic agents who are under risk. In Expected Utility Theory outcomes are weighted by their probabilities but Prospect Theory discusses that people dynamically give more or less weight to the outcomes depending on if it is a loss or a gain. This is called *certainty effect* [@kahneman1979]. The Certainty Effect causes people to behave differently whether they are going to lose or gain something. In the positive domain, people will prefer a small amount of a certain gain over a big gain which is merely probable (risk averse). On the contrary, in the negative domain, people will have a preference towards a big loss which is merely probable over a small certain loss (risk seeking).

Since the cognitive abilities of humans are limited they tend to focus on what is crucial to them. Therefore, in the information processing context and because of the existence of the negativity bias, it could be expected that people process negative cues more thoroughly than positive ones. Furthermore, people may develop much more advanced memory about negative entities. This can also cause high awareness to negative information [@baumeister2001; @taylor1991].

\newpage

# The Data and Research Methodology

## Data Collection

The data was scraped with a custom web scraper which was constructed with Selenium Framework for Python 3 [@python3]. The scraper acquired the customer reviews from IMDB and BestBuy. The former is the biggest platform for Movies and TV Shows, the latter is one of the biggest electronic retailers in United States of America. Both websites host a great amount of online reviews. After the data was collected it was further transformed and analyzed with the help of R Programming Language [@RCore; @Tidyverse; @Tidytext].

Online Consumer Reviews from IMDB were collected as an experience goods data since movies can be considered as pure-experience goods and their search attributes are very limited. It is nearly impossible to evaluate a movie before watching it. Furthermore, evaluations for a movie can both have subjective (e.g., personal attachment to the protagonist, atmosphere of the movie etc.) and objective aspects (e.g., writing, production quality etc.). There are also numerous studies that analyzed movies as experience goods [@nelson1970; @liu2014; @ghose2006; @baek2012; @lee2016].

BestBuy is a very big electronics retailer and this is why the website offers a sizable number of OCR for search goods. Contrary to experience goods, search goods usually have more objectively quantifiable traits. For example, resolution or screen size of a TV, RAM or GPU of a Laptop, storage or battery lifetime of a Smart Phone. Additionally, in the literature, TVs, Laptops and Phones were used as search goods [@mudambi2010; @sun2019; @baek2012; @ren2019]. Therefore, search goods data was collected from BestBuy as it was a very suitable candidate for this research.

From IMDB reviews of 22 movies were collected as experience goods. In total, 22 movies account for 31,863 reviews. From BestBuy, total of 383,357 reviews were collected as search goods. Specifically, 201,113 reviews for TVs, 130,270 for Laptops and 51,974 for Phones. However, in the data there are many reviews that were never got voted and probably they were never read because they got lost in the crowd of other reviews. To make sure that the analysis sample contains only the reviews which were actually read by potential buyers, a threshold of 5 was set for Total Votes. In other words, reviews that had less than 5 Total Votes were removed from the analysis data [@choi2020; @baek2012]. At the very end, the analysis data was consisted of 15,024 observations for experience goods and 8,042 observations for search goods. It should be noted that `r round(15024 / 31863, digits = 2) * 100` percent of the reviews had total votes of 5 or greater for experience goods, whereas only `r round(8042 / 383357, digits = 2)* 100` percent of the search goods were voted at least 5 times. Most of the search product reviews are not getting voted and even possible that they are not getting read at all. This might support the idea that people form more homogeneous opinions about search goods [@hong2014]. Once a helpful review is written most of the other reviews become redundant since other reviews most likely convey similar information. On the contrary, more attendance to the reviews can be observed for experience goods (movies) since nearly half of them were voted at least five times. This might be because people form heterogeneous opinions about experience goods [@hong2014].

As a data cleaning note, in IMDB data there were cases (20 observations to be exact) where the same reviews were written by different user names. All duplicates along with original reviews were removed because the reviews had different helpfulness ratios.

\newpage

## Exploratory Data Analysis

```{r Data_Prep_Experience}
# Experience Goods

## Read the Data

files <- dir()
files <- files[str_detect(files, "movie_.+")]

movies_data <- map_dfr(files, read_csv, show_col_types = FALSE)

## Removing reviews without Ratings
movies_data <- movies_data %>% 
  filter(!is.na(Rating)) 

## Creating Helpul and Total Votes Variables 
movies_data <- movies_data %>% 
  separate(Rating, into = c("Rating", "Total"), sep = "/", ) %>% 
  select(-Total) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Rating = as.double(Rating)) %>% 
  mutate(Helpfulness = Helpful_Vote / Total_Vote) %>% 
  mutate(Rating = Rating/2)

## Removing All the Duplicated Reviews including the originals since same reviews were written by differend user names
movies_data <- movies_data %>% 
  mutate(Rev_dup = duplicated(Review) | duplicated(Review, fromLast = TRUE)) %>% 
  filter(Rev_dup == 0)

# Words
## Creating Review ID
movies_data <- movies_data %>% 
  mutate(Doc_ID = row_number())

## Word Count
movies_agg <- movies_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

movies_data <- movies_data %>% 
  inner_join(movies_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- movies_data %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc") %>% 
               filter(!word %in% c("cage", "john","climax", "force", "king", "mad",
                                   "fury", "hardy","margin", "harry", "venom",
                                   "marvel", "stark", "iron", "vulture", "green",
                                   "goblin", "bale"))) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
movies_data <- movies_data %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = ~ifelse(is.na(.), 0, .) ))

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

movies_data <- movies_data %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))

## Freshness (Timeline)
movies_data <- movies_data %>% 
  mutate(Date = dmy(Date, locale = "english"))

first_dates <- movies_data %>% 
  group_by(Movie) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Movie, Date) %>% 
  rename(First_Date = Date)

movies_data <- full_join(movies_data, first_dates, by = "Movie") %>% 
  mutate(Freshness = as.integer(Date - First_Date))

```

```{r Data_Prep_Search}

# Search Goods
## Phone Data
phones_data <- read_csv("all_phone_reviews_15_03_22.csv", show_col_types = FALSE)

phones_data <- phones_data %>% 
  mutate(Product_Type = "Phone")

## Laptop Data
files <- dir()
files <- files[str_detect(dir(), "all_laptop_reviews.+")]

laptop_data <- map_dfr(files, read_csv, show_col_types = FALSE)

### Removing replicates that I couldn't detect earlier
laptop_data <- laptop_data %>% 
  filter(!Product_Name %in% c(
    'Samsung - Galaxy Book Pro 360 15.6" AMOLED Touch-Screen Laptop - Intel Evo Platform Core i7 - 16GB Memory - 1TB SSD - Mystic Navy',
    'ASUS - 14.0" Laptop - Intel Celeron N4020 - 4GB Memory - 64GB eMMC - Star Black - Star Black',
    'ASUS - 14" Chromebook - Intel Celeron N3350 - 4GB Memory - 32GB eMMC - Silver',
    'Samsung - Galaxy 13.3" 4K Ultra HD Touch-Screen Chromebook - Intel Core i5 - 8GB Memory - 256GB SSD - Mercury Gray'
  )) %>% 
  mutate(Product_Type = "Laptop")

## TV Data
tvs_data <- read_csv("all_tv_reviews_16_03_22.csv", show_col_types = FALSE)
tvs_data <- tvs_data %>% 
  mutate(Product_Type = "TV")

## Merging All 
search_goods <- phones_data %>% 
  bind_rows(tvs_data, laptop_data)

## Creating Total Votes variable Filtering Total Votes below 5

search_goods <- search_goods %>% 
  mutate(Total_Vote = Helpful + Unhelpful) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Helpfulness = Helpful / Total_Vote) %>% 
  mutate(is_Exp = 0)

search_goods <- search_goods %>% 
  mutate(Dupped = duplicated(Review)) %>% 
  filter(Dupped == 0)

## Word Counts
### Creating Review IDs
search_goods <- search_goods %>% 
  mutate(Doc_ID = row_number()) 

words_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

search_goods <- search_goods %>% 
  inner_join(words_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")%>% 
               filter(!word %in% c("cage", "john","climax", "force", "king", "mad",
                                   "fury", "hardy","margin", "harry", "venom",
                                   "marvel", "stark", "iron", "vulture", "green",
                                   "goblin", "bale"))) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
search_goods <- search_goods %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = ~ifelse(is.na(.), 0, .) ))


search_goods <- search_goods %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))


## Freshness (Timeline)
search_goods <- search_goods %>% 
  mutate(Date = as_date(mdy_hm(Date, locale = "english")))

first_dates <- search_goods %>% 
  group_by(Product_Name) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Product_Name, Date) %>% 
  rename(First_Date = Date)

search_goods <- search_goods %>% 
  full_join(first_dates, by = "Product_Name") %>% 
  mutate(Freshness = as.integer(Date - First_Date))
```

Exploratory Data Analysis (EDA) has no set of procedures to be followed. The way EDA is conducted almost always depends on the context of the research. @jebb2017 discusses that EDA encourages scientific replication if done properly. This section is going to be constructed with two notions in mind:

1.  Transparent data sharing for the sake of increased reproducibility value

2.  Helping readers build a better intuition about the model and its components through transparency

The goal of this section is not "confirmatory" whatsoever. Both dependent and independent variables are going to be introduced with an emphasis on their quantitative features such as their distributions and potential transformations which can be applied to the variable in question. In addition to this, bivariate relations between independent and dependent variables will be plotted in order to get an initial idea about the potential effects along with similar findings from the literature. Thus, in a sense, EDA in this section does not carry out the purpose of discovery, rather it merely puts the data on the spot for the sake of transparency.

### Dependent Variable

$$
Perceived\ Helpfulness = \frac{Helpful\ Votes}{Total\ Votes}
$$

The ratio of helpfulness is derived from the formula above. Below on Figure \@ref(fig:IV-Hist-Search) the distribution of the helpfulness for search goods can be observed along with the sample mean value which is represented as a vertical line.

```{r IV-Hist-Search, fig.cap="Distribution of the Helpfulness of Search Goods"}

search_goods %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Helpfulness)), size = 1, alpha = 0.6)+
  annotate(geom = "label",
           x = mean(search_goods$Helpfulness), y = Inf,
           label = paste("Mean:", round(mean(search_goods$Helpfulness), digits = 2)),
           vjust = 1, hjust = 1, size = 5)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = "Count")
  
```

As it can be seen the distribution is very skewed. Looks like in the electronics case, readers find some reviews very helpful. In fact, in the data there is one review which had over 170 helpful votes with zero unhelpful votes. On the other hand, distribution for experience goods on Figure \@ref(fig:IV-Hist-Exp) presents a different picture. This time helpfulness ratio is sort of normally distributed. There are not very helpful reviews. The most helpful review interval seems to be lying between 70% to 80%.

On a small note, the way the dependent variable was distributed for search goods may raise some questions about whether the standard errors will be calculated correctly or not in a linear regression method. @li2012 found out that given a relatively high sample size (N \> 3000) linear regression techniques produces sound estimates of coefficients. Since the sample size for search goods is over 8000 it might be assumed that the use of linear regression methods is acceptable.

```{r IV-Hist-Exp, fig.cap="Distribution of the Helpfulness of Experience Goods"}

movies_data %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Helpfulness)), size = 1, alpha = 0.6)+
  annotate(geom = "label",
           x = mean(movies_data$Helpfulness), y = Inf,
           label = paste("Mean:", round(mean(movies_data$Helpfulness), digits = 2)),
           vjust = 1, hjust = 1, size = 5)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = "Count")
```

@hong2014 states that consumers likely to form uniform opinions about search goods [@sun2019]. On the contrary, the same paper suggests that consumers' opinions are more diversified on experience goods. Intuitively, what these histograms show is the same. For instance, positive or negative attributes of search goods can be expressed in a limited number of ways. The storage space of a phone might be small or CPU of a laptop might not be fast as expected. Thus, one good review is enough to reveal the quality of a product. This might be the reason why such a big spike at 100% was observed on Figure \@ref(fig:IV-Hist-Search). In the movies (experience good) case on Figure \@ref(fig:IV-Hist-Exp), it can be seen that readers usually disagree with each other because the observations of the perceived helpfulness distributed around 50%. This might be because evaluation process of movies is rather subjective. For instance, one person might not like Spider-Man: Homecoming and another would love it. This might depend on how a person idealizes the Spider-Man or Peter Parker image in her mind which is completely subjective.

### Independent Variables

#### Review Rating (Review Extremity)

Review Rating is the star rating that was given by the reviewer as an evaluation of a product.

For both search and experience goods the bi-modality of the distributions can be observed on Figure \@ref(fig:Search-Rating) (A) and Figure \@ref(fig:Experience-Rating) (A) respectively. @hu2006 also pointed out a similar distribution concerning Online Product Reviews. This might mean that consumers usually tend to write product reviews when they are extremely happy or unhappy with the product.

Black vertical lines show the mean ratings for both product types. It is nice to see that averages are close to the neutral rating of 3. With this it can be made sure that the sample reviews do not address to positive or negative side only.

Because the data was taken from two different websites (BestBuy and IMDB), there are two different scales. For IMDB that is 1 to 10 and for BestBuy the scale is between 1 and 5. To make the interpretation of the regression coefficients uniform, IMDB's scale was divided by 2.

On Figure \@ref(fig:Search-Rating) (B) and Figure \@ref(fig:Experience-Rating) (B) the bivariate relationship between perceived helpfulness and review extremity can be seen. Once again, it must be emphasized that this analysis is not confirmatory. On figures, we can observe "U" type of relationship for both Search and Experience goods, though the non-linearity seems to be stronger with Search goods. This might suggest that on the average extreme reviews might be more helpful than moderate reviews [@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. On the other hand, it is observable that positive reviews on the average are more helpful than the rest. This might indicate that there is a linear positive relationship [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020].

```{r Search-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Search Goods."}
p1 <- search_goods %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  geom_vline(aes(xintercept = mean(Rating)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Rating), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Rating), digits = 2)),
           vjust = 1, hjust = 1, size = 4.5)+
  labs(y = "Count", x = "Review Rating")

p2 <- search_goods %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(method = "loess", color = "steelblue", size = 1.7)+
  labs(x = "Review Rating")

plots <-  p1 + p2

plots + plot_annotation(tag_levels = "A")
```

```{r Experience-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Experience Goods."}
p1 <- movies_data %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  geom_vline(aes(xintercept = mean(Rating)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Rating), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Rating), digits = 2)),
           vjust = 1, hjust = 1, size = 4.5)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = "Count", x = "Review Rating")

p2 <- movies_data %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 10)+
  labs(x = "Review Rating")

plots <-  p1 + p2

plots + plot_annotation(tag_levels = "A")
```

#### Review Length (Review Depth)

Review Length is the number of the words in a given review.

The exponential-like distribution of the number of words can be observed for both good types on Figure \@ref(fig:Search-Length) (A) and Figure \@ref(fig:Experience-Length) (A). This means that reviews become less prevalent as they get longer. This might suggest that logarithmic transformation can increase the model fit. For the transformed distributions see Figure \@ref(fig:Search-Length) (B) and Figure \@ref(fig:Experience-Length) (B). As it can be observed from the vertical mean lines the reviews of the experience goods tend to be longer than the reviews of the search goods.

The bivariate relationship between the number of words and Helpfulness seems to be varying for different product types. For Search goods on Figure \@ref(fig:Search-Length) (C) there is a clear positive relationship [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, this positive relationship appears to stop at a threshold [@schindler2012; @huang2015; @baek2012; @kuan2015; @eslami2018]. With this in mind, logarithmic transformation might make more sense (see Figure \@ref(fig:Search-Length) (D)) for modelling purposes. For Experience goods, on the other hand, there seems to be a weak negative relationship or no relationship at all [@chatterjee2020; @yang2019] (see Figure \@ref(fig:Experience-Length) (C) and (D)).

```{r Search-Length, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Word_Count)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Word_Count), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Word_Count), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 7)+
  labs(y = "Count", x = "Number of the Words")

p1_log <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  geom_vline(aes(xintercept = mean(Word_Count)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Word_Count), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Word_Count), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 8)+
  labs(y = "Count", x = "log10(Number of the Words)")

p2 <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 7)+
  labs(x = "Number of the Words")

p2_log <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Number of the Words)")

plots <-  (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Length, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Word_Count)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Word_Count), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Word_Count), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 7)+
  labs(y = "Count", x = "Number of the Words")

p1_log <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  geom_vline(aes(xintercept = mean(Word_Count)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Word_Count), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Word_Count), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 7)+
  labs(y = "Count", x = "log10(Number of the Words)")

p2 <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 7)+
  labs(x = "Number of the Words")

p2_log <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 7)+
  labs(x = "log10(Number of the Words)")

plots <-  (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Freshness of A Review

Review Freshness indicates how recent a review is. To engineer this variable following steps were taken:

1.  The first review date for a particular brand and model of a product (Movie, Phone, Laptop, TV) was taken.
2.  The first dates for particular products then subtracted from other reviews' dates under the respective brand and the model resulting an integer variable representing days.
3.  Bigger numbers represent more recent reviews.

Distributions of both product types resemble exponential distribution (see Figure \@ref(fig:Search-Freshness) (A) and Figure \@ref(fig:Experience-Freshness) (A)). Again a logarithmic transformation might be useful here (see Figure \@ref(fig:Search-Freshness) (B) and Figure \@ref(fig:Experience-Freshness) (B)) . Interestingly, even after 3 years from the release date some products are still getting reviewed from customers. Vertical mean lines indicate that the products in the experience goods sample are older than the products in the search goods sample because the margin between the first review for a given product and last review for a given product is wider.

On the scatter plots a weak relationship between perceived helpfulness and review freshness could be observed. The smoother line suggests both positive [@lee2014; @choi2020; @wu2021] and negative [@liu2014; @li2019; @zhou2019] linear relationship between two variables. This non-linearity may explain different findings in the literature.

```{r Search-Freshness, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Freshness)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Freshness), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Freshness), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 6)+
  labs(y = "Count", x = "Freshness of Review")

p1_log <- search_goods %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  geom_vline(aes(xintercept = mean(Freshness)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Freshness), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Freshness), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 6)+
  labs(y = "Count", x = "log10(Freshness of Review + 1)")

p2 <- search_goods %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 6)+
  labs(x = "Freshness of Review")

p2_log <- search_goods %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 6)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

```{r Experience-Freshness, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Freshness)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Freshness), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Freshness), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 6)+
  labs(y = "Count", x = "Freshness of Review")

p1_log <- movies_data %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 1)+
  geom_vline(aes(xintercept = mean(Freshness)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Freshness), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Freshness), digits = 0)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 6)+
  labs(y = "Count", x = "log10(Freshness of Review + 1)")

p2 <- movies_data %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 6)+
  labs(x = "Freshness of Review")

p2_log <- movies_data %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 6)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

#### Proportion of Negative Words

Negative words were derived by using NRC Word-Emotion Association Lexicon [@Mohammad13]. A lexicon is basically a dictionary of words. These words might have similar emotions or sentiments. In this case, negative words lexicon was used in order to engineer this variable. A sample of negative words from the lexicon can be observed below.

> *"caricature, superstition, compost, misconception, grief, violation, scrub, omen, erratic, porcupine"*

$$
Negative\ Word \% = \frac{Number\ of\ Matched\ Negative\ Word}{Total\ Words\ in\ the\ Review} 
$$

These steps were taken in order to create this variable:

1.  Words in a review were tokenized.
2.  Tokenized words were matched with the words in the lexicon.
3.  Matches were counted and divided by the total number of words with the respective review then multiplied by 100 in order to make regression results more interpretable.
4.  Logarithmic transformation was applied in order to scale the variable in a more normalized way.

On Figure \@ref(fig:Search-Neg) (A) and Figure \@ref(fig:Experience-Neg) (A) the distributions of the variable for both good types can be seen. On plots, X-axis labels were given in the logarithm of base 10 so 1.00 on the X-axis implies that 10% of the review was negative words. For example in this case the mean negative proportion rate is 2,3% ($10^{0.37}$) for search goods and 3,7% ($10^{0.57}$) for experience goods. It should be noted that `r round(mean(search_goods$negative == 0), digits = 2)` percent of the reviews for search goods do not have any negative words in them. For experience goods this number is `r round(mean(movies_data$negative == 0), digits = 2)` percent.

On Figure \@ref(fig:Search-Neg) (B), non-linear relationship between perceived helpfulness and the proportion of negative words can be observed for search goods. This might suggest that too much negative information might fall into slander rather than informing the customer about the potential risks of the product. On the other hand, somewhat weak negative linear relationship between perceived helpfulness and proportion of negative words can be seen for experience goods on Figure \@ref(fig:Experience-Neg) (B).

```{r Search-Neg, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Search Goods"}
p1 <- search_goods %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(negative)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$negative), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$negative), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Negative Words)")

p2 <- search_goods %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Neg, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Experience Goods"}
p1 <- movies_data %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(negative)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$negative), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$negative), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Negative Words)")

p2 <- movies_data %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Proportion of Positive Words

$$
Positive\ Word \% = \frac{Number\ of\ Matched\ Positive\ Word}{Total\ Words\ in\ the\ Review} 
$$

This variable was engineered in the same manner with its negative words counterpart. In this case, however, reviews of both good types have more positive words in them proportionally when compared to negative words. `r round(mean(search_goods$positive == 0), digits = 2)` percent of the reviews of search goods do not have any positive words in them and this number is only `r round(mean(movies_data$positive == 0), digits = 2)` for experience goods. Mean proportion values are 4,4% ($10^{0.64}$) and 5,2% ($10^{0.72}$) for search and experience goods respectively. Except for zero values both distributions are looking quite normal (see Figure \@ref(fig:Search-Pos) (A) and Figure \@ref(fig:Experience-Pos) (A)).

*A sample of positive words from EmoLex:*

> *"covenant, jovial, score, foundation, affirmation, reliable, mending, garden, luxurious, wit"*

On scatter plots (see Figure \@ref(fig:Search-Pos) (B) and Figure \@ref(fig:Experience-Pos) (B)), relationships between perceived helpfulness and proportion of positive words are looking very similar to their negative counterparts. Furthermore, these plots are kind of similar to perceived helpfulness versus number of words plots (see Figure \@ref(fig:Search-Length) (B) and Figure \@ref(fig:Experience-Length) (B)). In a sense, both negative and positive proportions imply the number of words. As the proportion increases so does the number of words. That is why aforementioned figures show similar relationships between variables. On higher dimensions these relationships might be different. That is why Exploratory Data Analysis should never be conducted in a confirmatory sense.

```{r Search-Pos, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Positive Words for Search Goods"}
p1 <- search_goods %>% 
  ggplot(aes(positive))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(positive)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$positive), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$positive), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Positive Words)")

p2 <- search_goods %>% 
  ggplot(aes(positive, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Positive Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Pos, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Positive Words for Experience Goods"}
p1 <- movies_data %>% 
  ggplot(aes(positive))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(positive)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$positive), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$positive), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Positive Words)")

p2 <- movies_data %>% 
  ggplot(aes(positive, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Positive Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

### Control Variables

#### Total Votes

The number of Total Votes is going to be used as a control variable since 80% Perceived Helpfulness can arise from both 4 Helpful votes over 5 Total votes or 80 Helpful votes over 100 Total votes [@mudambi2010; @liu2014; @choi2020].

From Figures \@ref(fig:Search-Total) (B) and \@ref(fig:Experience-Total) (B) it can be observed that logarithmic transformation scales the variable in a better way for both good types. This is also good for linear models. Distributions for both good types are looking similar. There is a little difference though. Movies got voted many more times than search goods. This makes sense because relatively older movies are present in the data. In the case of electronics, it is hard to find reviews for older devices because they are not sold anymore so it is impossible to vote on them. This explains why there are so few amount of highly voted reviews for search goods.

```{r Search-Total, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Total Votes for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Total_Vote))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Total_Vote)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Total_Vote), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Total_Vote), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 7)+
  labs(y = "Count", x = "Total Vote")

p1_log <- search_goods %>% 
  ggplot(aes(Total_Vote))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  geom_vline(aes(xintercept = mean(Total_Vote)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(search_goods$Total_Vote), y = Inf, 
           label = paste("Mean:", round(mean(search_goods$Total_Vote), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 7)+
  labs(y = "Count", x = "log10(Total Vote)")

p2 <- search_goods %>% 
  ggplot(aes(Total_Vote, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 7)+
  labs(x = "Total Vote")

p2_log <- search_goods %>% 
  ggplot(aes(Total_Vote, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 7)+
  labs(x = "log10(Total Vote)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

On Figures \@ref(fig:Search-Total) (C) and (D) the smoother line may look like a straight horizontal line because the plot is small but there exist a weak negative slope. This might indicate two things. There is no association between these two variables or there is a negative relationship.

Figure \@ref(fig:Experience-Total) (C) and (D) show a different story. Figures might indicate that there is a positive relationship between these two variables.

Nonetheless, it should be noted that the variable Total Votes is the denominator part of the dependent variable Perceived Helpfulness so any interpretations to these plots should be done with care.

```{r Experience-Total, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Total Votes for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Total_Vote))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  geom_vline(aes(xintercept = mean(Total_Vote)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Total_Vote), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Total_Vote), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_continuous(n.breaks = 7)+
  labs(y = "Count", x = "Total Vote")

p1_log <- movies_data %>% 
  ggplot(aes(Total_Vote))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 1)+
  geom_vline(aes(xintercept = mean(Total_Vote)), size = 1, alpha = 0.6)+
  annotate(geom = "label", x = mean(movies_data$Total_Vote), y = Inf, 
           label = paste("Mean:", round(mean(movies_data$Total_Vote), digits = 2)),
           vjust = 1, hjust = 0, size = 4.5)+
  scale_x_log10(n.break = 6)+
  labs(y = "Count", x = "log10(Total Vote)")

p2 <- movies_data %>% 
  ggplot(aes(Total_Vote, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_continuous(n.breaks = 7)+
  labs(x = "Total Vote")

p2_log <- movies_data %>% 
  ggplot(aes(Total_Vote, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue", size = 1.7)+
  scale_x_log10(n.breaks = 6)+
  labs(x = "log10(Total Vote)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

### Correlation Table

Before progressing to the modelling section, it would be a good practice to check the correlations between independent and control variables in order not to cause any multi-colinearity problems. As it can be seen on Figure \@ref(fig:Correlations) there are not any major correlations between independent and control variables for both good types.

The relatively high positive / negative correlation between the Review Rating and Positive / Negative word proportions might give a hint about the usefulness of the lexicon. As the Review Rating gets higher the review contains more positive words and, on the other hand, low rated reviews have more negative words in them. This makes sense intuitively.

```{r Correlations, fig.cap="Correlations between Independent Variables.", fig.height=8}



p1 <- search_goods %>% 
  select("Review Rating" = Rating,
         "Review Length" = Word_Count, Freshness,
        "Positive Proportion" = positive,
        "Negative Proportion" = negative,
        "Total Votes" = Total_Vote) %>% 
  mutate(`Review Length` = log10(`Review Length`),
         `Total Votes` = log10(`Total Votes`)) %>% 
  ggcorr(label = TRUE, hjust = 0.75, nbreaks = 5,low = "steelblue", mid = "grey95",
         high = "darkred", label_round = 2, layout.exp = 1)+
  labs(title = "Search Goods")+
  theme_void()


p2 <- movies_data %>% 
  select("Review Rating" = Rating,
         "Review Length" = Word_Count, Freshness,
        "Positive Proportion" = positive,
        "Negative Proportion" = negative,
        "Total Votes" = Total_Vote) %>% 
  mutate(`Review Length` = log10(`Review Length`),
         `Total Votes` = log10(`Total Votes`)) %>% 
  ggcorr(label = TRUE, hjust = 0.75, nbreaks = 5,low = "steelblue", mid = "grey95",
         high = "darkred", label_round = 2, layout.exp = 1)+
  labs(title = "Experience Goods")+
  theme_void()

(p1 / p2) +
  plot_layout(guides = "collect")
```

\newpage

## Hypothesis Development

@korfiatis2012 found that neutral rated reviews (3 out of 5 stars) imply indifference to readers. This means that prior information of the potential buyer can not be influenced by these neutral reviews and likely to be found less helpful compared to extreme reviews (1 or 5 stars) [@wang2018; @liu2014; @cao2011; @ghose2006]. As the reviews get more extremely rated they might strengthen or sway prior information that a potential consumer has, making them more helpful. This extremeness has more objective nature for Search Goods since there are fewer ways to express pleasantness or unpleasantness for a particular product. For instance a bad design in a laptop might cause fans to make much more noise than they should normally do. This information along with a very low rating may signal that there are some inherent problems with the particular product, making the feedback more helpful. On the other hand, for Experience Goods and in the specific case of movies, opinions for a given product might be much more subjective. For example, a reviewer might find a movie "childish" and give it one star, another reviewer might find the movie "light-hearted and colorful" and give the movie 5 stars, making the helpfulness subjective and decreasing its usefulness. Furthermore, for experience goods, it was found that opposite extremes to a potential buyer's prior expectations are not impactful on the decision making process [@hoch1989]. In other words, prior expectations play a bigger role in decision making process for experience goods. Therefore it is hypothesized that:

***H1**: There is a U-Shaped relationship between Perceived Helpfulness and Review Rating. This U-Shape is stronger/weaker for Search/Experience Goods.*

In order to decrease the uncertainty in the most optimal way, search costs must be reduced. In Section \@ref(search-and-experience-goods-classification) it was mentioned that the action of search was costlier for Experience Goods than Search Goods. Furthermore, people tend to have similar opinions about Search Goods [@hong2014]. This might mean that information cues are much more uniform for Search Goods [@huang2015; @baek2012]. In other words, there are limited ways to construct a review because search attributes are more objective. Therefore, there might be a positive linear relationship between Perceived Helpfulness and Review Length because longer reviews can help the consumers in their decision making process by increasing the amount of information they have. However, since the attributes of electronics are objectively identifiable and limited (e.g., storage space of a phone, RAM of a laptop, color depth of a TV etc.) there are not much to review for a reviewer. Because of this, there might be diminishing marginal returns to Review Length. After some point more information can cause confusion and may render the review unfocused [@schindler2012; @huang2015]. On the contrary, for Experience Goods, people have tendency to form heterogeneous opinions [@hong2014]. This means that movie reviews are much more subjective by nature. Since there are less objectivity, a helpful review to one person might not be helpful to another. This might decrease the average responsiveness of the Perceived Helpfulness to the Review Length. However, since there are other ways to objectively evaluate a movie such as the quality of its effects, movie's budget and how well this budget was utilized, a weaker positive linear relationship is expected. Therefore it is hypothesized that:

***H2**: There is a positive relationship between the logarithm of the Review Length and the Perceived Helpfulness. This effect is stronger for Search Goods than Experience Goods*.

As it was mentioned in Section \@ref(negativity-bias-theorem-and-loss-aversion), people tend to try to minimize their losses. Because of this, negative cues draw more attention than positive ones since negative cues may imply potential losses. @guo2019 found out that people spent more time when evaluating negative reviews. This may suggest a linear interaction between Review Extremity and Review Length. Since people take more time to evaluate negative reviews, this may indicate that the effect of Review Length is stronger when Review Rating is low. And when a review is longer it is perceived as more helpful for negatively rated reviews than positively rated reviews. In this case, it is hypothesized that:

***H3**: There is a negative linear interaction effect between Review Rating and the logarithm of the Review Length for both Search and Experience Goods.*

Potential buyers may find more recent reviews helpful because they may want to get updated information on a product. This might especially be true for products which are on the market for more than a year. For instance, a laptop which was produced in 2018 could be a good choice for that year but for year 2021 it is probable that its features and specifications would be below average. A similar behavior might be possible for movies as well. However, this time reviewers might need to take their time to digest and evaluate the movie. A review which is carefully constructed may be posted late, making it more recent compared to a review which was hastily written. Furthermore, some popular movies which are considered as classics may attract reviews even way later than they were first released. Since some websites may employ algorithms that may push newer reviews to the top in order to be able to increase the interaction on their platforms, these kind of reviews may attract more helpful votes. Thus, it is hypothesized that:

***H4**: There is a positive relationship between Perceived Helpfulness and Freshness of a Review for both Search and Experience Goods.*

So far it should be understood that people try to cut down their losses. That is why negative cues draw more attention than positive ones [@kahneman1979; @schindler2017]. Additionally, people find negative information more credible and persuasive than positive information [@ito1998; @Kanouse1984ExplainingNB]. It is known that Review Rating is the satisfaction level of a customer towards a product. However, Review Rating itself is not enough to convey the information about how a customer expresses her satisfaction / dissatisfaction towards a product. The way a customer expresses her opinion about a product might have an effect on the perceived helpfulness of a review. Reviews with negative sentiments may bring out the risks associated with the purchase better than reviews with positive sentiments. Since in this study movies and electronics are compared, it is costlier to make a wrong decision on Search Goods than Experience Goods. Therefore, it is hypothesized that:

***H5**: There is a positive relationship between the Proportion of Negative Words and the Perceived Helpfulness. This effect is stronger / weaker for Search/Experience goods.*\
***H6**: There is a negative relationship between the Proportion of Positive Words and the Perceived Helpfulness for both product types.*

## Research Methodology

Tobit model is also named as Censored Regression model. Censored data arises when there exist lower (censored from below) or upper (censored from above) or both lower and upper boundaries for the dependent variable. This type of data is fairly prevalent in economics. For instance, in consumer economics there is no such thing as "negative consumption" or in labor economics there is no such thing as "negative work hours". Furthermore, there might be cases where some really high values could be recorded as "50,000 or more" because of the data collection design. Ordinary Least Squares can not take account of the censored observations in the data because it extrapolates lower and upper ends when it fits the model and this leads biased estimations.

Tobit regression approaches censored dependent variable problem from two angles. In a sense Tobit regression could be considered as the summation of Probit regression and Truncated regression. The probit side considers if a particular observation is above or below the given limit. If the observation, for example, is below the lower limit then it must be equal to the lower limit. If it is higher than the lower limit the problem turns to a normal regression. In this case, Perceived Helpfulness can be expressed such as this:

$$
y=
\begin{cases}
  0   & \text{if  $ \ y^* \le0$} \\
  y^* & \text{if $ \ 0< y^* <1$} \\
  1   & \text{if $ \ y^* \ge 1$}
\end{cases}
$$

Tobit Regression is selected as the analysis method because of three reasons. First of all, the dependent variable, perceived helpfulness, is defined in a continuous range from 0 to 1 since perceived helpfulness is a percentage as mentioned above. The second reason as stated by @mudambi2010 is the potential selection bias in the sample. Unfortunately, it is not possible to know how many people read a particular review. Neither BestBuy nor IMDB disclose this information. Because of this, being included in a sample could be correlated with independent variables. For instance, potentially more voted reviews could attract more readers or readers could be drawn to reviews which have certain ratings. Lastly, there are many studies from the literature that used Tobit Regression as their research methods [@mudambi2010; @korfiatis2012; @huang2015; @choi2020; @ren2019].

\newpage

# Results

Tobit regression results are presented at Table \@ref(tab:Reg-Results-Tobit) alongside with OLS results as a benchmark at Table \@ref(tab:Reg-Results-OLS) [@mudambi2010; @kuan2015; @choi2020].

Hypothesis 1 proposed a U-Shape for the relationship between Review Extremity and Perceived Helpfulness. For Search Goods this is true because the linear Rating Extremity coefficient (-0.061, p \< 0.01) has a negative sign and the quadratic Rating Extremity coefficient (0.036, p \< 0.001) has a positive sign. The presence of the significant linear interaction effect between Review Extremity and Review Length accentuates the linear Rating Extremity coefficient (-0.043, p \< 0.001). This means that as reviews get longer negative extreme reviews become more useful. For Experience Goods, on the other hand, both linear (0.026, p \< 0.001) and quadratic (0.006, p \< 0.001) Review Extremity terms are positive. A U-Shaped function can not be observed. However, the significant interaction effect between Review Length and Review Extremity (-0.015, p \< 0.001) suggests that for longer reviews the linear term turns to negative. This means that the function takes a U-Shape after a certain review length. It can also be observed that Search Goods have stronger coefficients than Experience Goods meaning the expected U-Shape is more prominent for Search Goods. Therefore, it can be said that for shorter reviews Hypothesis 1 is partially supported and for longer reviews it is fully supported.

A positive linear relationship between the logarithm of Review Length and Perceived Helpfulness was proposed with Hypothesis 2. A strong relationship between Review Length and Perceived Helpfulness is observed (0.404, p \< 0.001) for Search Goods. This effect is moderated by Review Extremity since the interaction effect is significant with a negative sign (-0.043, p \< 0.001). As review rating increases the responsiveness of the Perceived Helpfulness to Review Length decreases. This means that for positive extreme reviews relatively shorter texts are more helpful. Logarithmic form also proves that an additional word in a review shows diminishing returns to helpfulness. Experience Goods somewhat deviates from the results of Search Goods. There is a weak positive relationship between Review Length and Perceived Helpfulness (0.021, p \< 0.01). However, this positive relationship quickly turns to negative for higher ratings because of the presence of the interaction term between Review Length and Review Extremity (-0.015, p \< 0.001). For lower ratings Hypothesis 2 is fully supported but as the Review Extremity increases this support weakens.

For both good types the interaction effects are significantly negative (-0.043, p \< 0.001 ; -0.015, p \< 0.001). This means that as Review Rating increases longer reviews become less helpful, supporting the idea of people spending more time evaluating negative information because it takes more time to read a longer review. However, there are nuances too. For Search Goods, the effect of the Review Length is always positive whether or not Review Rating is low or high. On the contrary, for Experience Goods, after some point of Review Rating longer reviews become "unhelpful" because the sign of the Review Length turns to negative. In this case Hypothesis 3 is supported wholly for Search Goods and somewhat supported for Experience Goods.

Hypothesis 4 is fully supported. Recent reviews seem to be more helpful for both good types (0.029, p \< 0.001 ; 0.038, p \< 0.001).

The results for Negative Word Proportion is quite interesting because it shows different characteristics for different product types. As expected, for Search Goods, Negative Word Proportion has a significant positive coefficient (0.061, p \< 0.001). The presence of negative words indeed increases the perceived helpfulness. Contrary to this, there is a negative relationship between negative words and perceived helpfulness for Experience Goods (-0.021, p \< 0.001). Hypothesis 5 is only supported for Search Goods and not supported for Experience Goods.

Positive Word Proportion is only significant for Search Goods (-0.056, p \< 0.001). The presence of positive words has a negative effect on perceived helpfulness just as expected. The coefficient for Experience Goods show the expected sign too but it is statistically insignificant (-0.007, p \> 0.5). With this result, Hypothesis 6 is again only supported for Search Goods and not supported for Experience Goods.

The result of the only control variable should also be reported. It seems that total amount of votes that a review gets has a negative relationship (-0.157, p \< 0.001) with perceived helpfulness for Search Goods. The opposite is true for Experience Goods (0.123, p \< 0.001).

```{r}
search_tobit <- tobit(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = search_goods,
                     left = 0, right = 1)

experience_tobit <- tobit(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = movies_data,
                     left = 0, right = 1)


search_lm <- lm(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = search_goods)

experience_lm <- lm(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = movies_data)

```

```{r Reg-Results-Tobit, results='asis'}

stargazer(list("Search Tobit" = search_tobit,
               "Experience Tobit" = experience_tobit),
          type = "latex",
          notes.align = "l",
          header = FALSE,
          model.numbers = FALSE,
          column.labels = c("Search", "Experience"),
          covariate.labels = c("Rating", "Rating Squared", "log10(Review Length)",
                               "log10(Total Vote)", "log10(Freshness + 1)",
                               "Positive Proportion",  
                               "Negative Proportion", "Rating x Review Length"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          label = "tab:Reg-Results-Tobit",
          title = "Tobit Regression Results")
```

```{r Reg-Results-OLS, results='asis'}

stargazer(list("Search Linear" = search_lm,
               "Experience Linear" = experience_lm),
          type = "latex",
          notes.align = "l",
          header = FALSE,
          model.numbers = FALSE,
          column.labels = c("Search", "Experience"),
          covariate.labels = c("Rating", "Rating Squared", "log10(Review Length)",
                               "log10(Total Vote)", "log10(Freshness + 1)",
                               "Positive Proportion",  
                               "Negative Proportion", "Rating x Review Length"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          label = "tab:Reg-Results-OLS",
          title = "OLS Regression Results")
```

\newpage

# Discussion

The results show a clear contrast between the objective nature of search goods versus the subjective nature of experience goods which once again validates the notion of Search and Experience goods classification that was developed by @nelson1970. It should not be ignored that this contrast is only relevant for the case of electronics versus movies since the available data only included these products. Electronics such as laptops or phones carry very strong search attributes whereas movies can be classified as "pure experience" goods which can be considered as a special case [@liu2014].

Negativity Bias Theorem and Prospect Theory had a strong support in the case of search goods. The regression results showed that the reviews which had low ratings and/or reviews which had more negative words in them were found to be more helpful on the average. Additionally, Perceived Helpfulness has higher responsiveness to Review Length when a review has a low rating. This means that people focus more on negative cues than positive cues because the marginal value of an additional negative information is higher. This way consumers are able to cut down potential sure losses. A similar result was reflected by the negative coefficient of the proportion of positive words in a review. This shows that reviews with positive sentiments do not offer diagnostic value for consumers. Nonetheless, the U-Shape of the Review Rating variable also suggests positivity bias [@hoorens2014]. An explanation to this could be that certain brands or products might be favored by some customers and these customers might selectively tend towards positively rated reviews and find them helpful just because they love the brand [@zhu2021]. The reason why there is a divergence between the "positively rated review" and "a review with positive sentiment" might be explained by this. A consumer who has brand or product inclinations wants to be persuaded to have the product so this particular consumer does not really care about what is written in the review, he might skim through positive reviews until he feels persuaded. Since a preference towards a brand is subjective it might be speculated that a strong brand image could embed experience attributes into search goods. A similar notion was discussed by @mudambi2010 where they classified Apple iPod as an experience good. That is although an MP3 player such as iPod has many objective search attributes such as storage capacity and battery life, what makes this particular product preferable most of the time is the product image that was created by Apple.

The rationality why the statistical results for search goods came out just as expected can be accredited to their objective features. Properties of electronics are easily evaluated prior purchase. This means that it is comparatively easier to collect information on search goods than experience goods. Other than online consumer reviews on a retailer's website, a potential buyer has an arsenal of product reviews which are available on tech enthusiastic websites where electronics are reviewed by professionals. Furthermore, some channels on YouTube offer product reviews where the reviewer inspects the product in question on a camera. Thus, the potential buyer can gather a great amount of information even before accessing to the online retailer's website.

A similar strong support for Negativity Bias can not be observed in the case of movies. Only evidence for Negativity Bias was the significant linear interaction term between Review Length and Review Rating. According to this result the U-Shaped function of Review Rating is only observable when Review Length is high enough. A similar situation is the case for search goods as well but the interaction term does not interfere with the signs of the other coefficients so no matter the Review Length is a U-Shaped function is observed for search goods. However, this is certain that as the review rating gets higher the review loses its diagnostic value for both for experience and search goods because the coefficient of the Review Length weakens. This means that potential buyers require more elaboration when a review is rated low. For experience goods, high rated reviews become more helpful on the average when reviews are short. This also might be a sign for positivity bias where people pay more attention to reviews which were aligned with their own feelings [@zhu2021]. Since experience attributes are naturally subjective it is logical to think that readers look for reviews that they agree.

A number of reasons could be speculated about why the Hypothesis 5 and Hypothesis 6 did not hold for experience goods. The expectation was observing a negative significant coefficient for proportion of positive words and a positive significant coefficient for proportion of negative words. At the very least it can be said that the existence of positive words in a movie review does not offer a significant information value according to regression results. Thus, it is hard to say there is a strong deviation from Hypothesis 6. On the contrast, Hypothesis 5 for experience goods was not supported at all.

One reason of this might be that the NRC Word-Emotion Association Lexicon which was used to mine the negative words in the reviews was not suitable for the movie reviews. For instance, the word "*cage"* is a negative word in the lexicon and "*marvel*" is a positive word. One or more movies in the sample include movies where "Nicholas *Cage*" had the starring role. There are also "*Marvel* Cinematic Universe" movies present in the data. Cases like these may create noise for the variable and the model because aforementioned words were used frequently and outside of the context of the lexicon. To test this problem, negative and positive words such as "*cage, john, climax, force, king, mad, fury, hardy, margin, harry, marvel, stark, iron, vulture, green, goblin, bale*" were filtered out of the lexicon for both good types. Neither coefficient signs nor coefficient significances were changed. However, it should be pointed out that the magnitude of the coefficient of negative words for experience goods was weakened slightly. Further filtering was not pursued because of potential dangers of hypothesis fishing. (The results in Tables \@ref(tab:Reg-Results-Tobit) and \@ref(tab:Reg-Results-OLS) are the results with the filter.)

Another reason may arise from the subjective nature of the experience attributes of the movies. Negative words, in the context of electronics, may have a narrower range of meanings because what they usually try to convey directly relates the to quality of the good itself. The quality of the good is easily accessible because of the fact that electronics have dominant search attributes. Thus, negative words probably translates as potential monetary risks rather than affecting emotions. A risk might be exemplified as purchasing a sub-par performing laptop with a given budget, in other words getting sub-optimal utility. Furthermore, electronics are usually bought to be used for a comparatively long period of time. A smartphone or a television can serve its owner for years so negative cues which relate to below average durability are very important. Buying a sub-durable product for a given budget can only increase the chances that the consumer will have to buy the same type of product prematurely in the future. On the other hand, the monetary risks associated with a bad movie are relatively lower. Consumers can go to cinema, buy a DVD, rent the movie on YouTube or access the movie as a part of their subscription plan on platforms such as Netflix. Subscription or ticket fees are not the only cost for the consumer. In order to consume the product the consumer must spend the time that is equal to the duration of the movie. Of course, the consumer can stop watching the movie anytime if it is too unbearable for her so she does not waste any more time. Because of these reasons the money spent on a movie might be irrelevant to an average consumer since accessing a movie is relatively cheaper than owning an electronic device. Therefore, negative words in a movie review may not alert the consumer for potential monetary losses. Instead, negative words could render a review in question unpleasant because emotions are priority in the case of consumption of movies.

There are still points to be made about the results of the movies. In the Section \@ref(search-and-experience-goods-classification) it was mentioned that consumers choose from two different strategies when they want to acquire information about a particular product. They might choose to experience the good or they can undertake search in order to collect information. It was pointed out by @nelson1970 that word-of-mouth type of information is viable when it is much more costlier to "experience" the good than undertaking a search process. It is probable that, in the case of movies, watching the movie (experiencing it) isn't that much costlier or maybe less costlier than gathering information about that particular movie and then making a decision on watching it. Thus, "search" might not be very justifiable for movies. This might explain why electronic word-of-mouth (Review Length) showed rather weaker and erratic (positive for lower ratings, negative for higher ratings) results.

Unexpected and different results are common in the literature of Review Helpfulness [@fan2021]. This work and the rest of the literature suffer from certain biases that the data imposes. Many other limitations may arise from potential omitted variables.

Firstly, as @mudambi2010 pointed out that potential selection bias exists in the data. It is not possible to know how many people actually read a particular review. Additionally, there is a strong possibility that as a review gets more votes this is going to draw more attention to the particular review. This can cause a positive feedback loop and this feedback loop can increase the chances that an observation making into the sample which leads to a selection bias. To fight with this, researchers usually resorted to setting a threshold of total votes where reviews with small amount of votes were filtered out of the sample. Furthermore, a multi-level regression model was developed by @kuan2015 in order to overcome the selection bias. In the first level of the regression the researchers modeled the binary case of whether a review gets voted or not. After what makes a review voted was established, they progressed to the next stage of modelling the review helpfulness. Along with the multi-level model results they also shared OLS results where non-voted reviews were filtered from the data. Most of the significance and coefficient signs were not different but coefficients differed in magnitude.

Secondly, as pointed out in Section \@ref(literature-review), there were many different research designs present in the literature. Let alone different methodologies, even the utilizations of similar variables were contrasting. For instance, review extremity was employed in three different ways by researchers. This study and some others use the review ratings as it is. Another group of studies took the differences of review ratings from the mean rating of a given product. The resulting variable is usually called Review Inconsistency. The last group of researchers utilized a dummy variable for neutral reviews (3 out of 5) and extreme reviews (1,2,4,5 out of 5). In some studies these dummy variables were extended to negative and positive extreme dummies as well. The differences are not limited to Review Rating. The way the dependent variable was used, Perceived Helpfulness, diverged too. In some circumstances only absolute Helpful votes were used along with Negative Binomial Regression. Furthermore, some studies converted the percentage of the helpfulness into a binary problem where reviews with over 50% helpfulness score was considered as a helpful review. As a last example to different applications, scale transformations (logarithmic transformation, etc.) were applied liberally as well. All in all, it is clear that there is no standard approach in the literature which might be the reason why there are many conflicting results.

Potential omitted variables such as consumer's buying motivation or his/her trust towards strangers may also explain the conflicting or unexpected results as pointed out by @zhu2021. A person who is more agreeable will high likely find many more reviews helpful on the average. Additionally, buying motivation might play an important role in explaining the review helpfulness. Above, it was speculated that some consumers might have tendency towards certain brands or products. Favored brands might not be the only factor that affects the consumer motivation. For instance, a consumer with limited amount of budget may find positive reviews of products that have suitable prices helpful. Thus the product price relative to the consumer's budget can also be used as an indicator.

\newpage

# Conclusion

Online Consumer Reviews data were collected from two different sources (IMDB and BestBuy) and analyzed within the context of consumer uncertainty in this study. Negativity Bias Theorem, Prospect Theory and Search / Experience goods paradigm from information economics were used as theoretical engines to explain the information gathering process of an average consumer. Results showed similarities from the existing Review Helpfulness literature. However, once again it should be noted that the literature has many conflicting results.

This study contributes to the literature as an additional empirical work with a unique BestBuy review data and a unique combination of movie reviews from IMDB. In addition, to my knowledge, this is the first study which analyzed the linear continuous interaction between the Review Length and the Review Rating in terms of Negativity Bias Theorem. Additionally, there is a lack of emphasis on the sentiment analysis in the existing literature. By employing positive and negative sentiments in terms of Negativity Bias and Prospect Theory this study shows the viability of the sentiment analysis in this context.

Goods with very strong search or experience attributes were chosen in order to compare the information gathering process in the clearest way possible. The results prove the fact that information gathering is easier for search goods than experience goods. This means that consumers face less uncertainty against search goods. The results are only generalizable to laptops, phones, televisions and movies since different goods have different mix of search and experience attributes.

Findings show the importance of information sharing for an average online consumer at least for the search goods. Since helpful reviews aid consumers deal with their uncertainties by helping them acquire information concerning the quality of a given product, attendance to information sharing either by writing reviews or voting if the reviews are helpful or not is strongly advised. Search goods results at Table \@ref(tab:Reg-Results-Tobit) can be used as a guide for writing a review. The guidelines can be listed such as this:

1.  Reviewers should only write a review when their opinions of a given product is rather extreme.
2.  Reviewers should share all the information and the experience they accumulated about the product without making it too long because it was observed that helpfulness shows diminishing returns to additional words. The review should be clear and to the point.
3.  Reviewers should not refrain from using negative words as long as it objectively identifies something wrong or lacking with the product which is being reviewed.
4.  When reviewers are reviewing a product which they are extremely unhappy about they should give more details and elaborate, on the opposite, when they are content they should write relatively shorter reviews.
5.  Since recent reviews were found to be more helpful on the average, sharing updated information, i.e, writing new reviews is important and encouraged.

With this way consumers would be able to purchase the best good available in the market with respect to their budgets, i.e., maximizing their utilities. This collective behavior may even impact social welfare positively since economic agents are making much more informed decisions. Unfortunately, similar implications can not be made for experience goods, specifically for movies, since the realized results were mostly different than what was hypothesized. Thus, this study can not offer any suggestions for movie reviews.

There are some implications for the future studies. Firstly, aside from lexicon based sentiment analysis it is also possible to utilize deep learning methods to mine opinions from a text. One advantage of the deep learning method over a lexicon is its versatility. A negative word can change from one context to another and training a deep learning method is more robust to this type of contextual changes whereas with lexicon based methods a negative word is a negative word no matter the context is. Next, future studies should consider collecting data from different regions and cultures. There might be some differences on how consumers process and react to the information in order to deal with uncertainty. Additionally, researchers can dig deeper into search and experience goods paradigm. It is well established in the literature that product type affects the information gathering process. However, there might be differences within product types depending on their search and experience attributes. An objective way to identify and index experience and search attributes may also shed some light to the conflicting results in the literature. Lastly, since Online Consumer Reviews data is ever growing more researchers should incorporate Exploratory Data Analysis into their works. Reviews and how helpful these reviews are perceived by potential buyers is relatively a new topic. In addition, new ways to analyze text data are being developed since the area of Natural Language Processing is quite hot so Online Consumer Reviews offer a great potential for new discoveries from the data for different domains. This increases the value of the Exploratory Data Analysis as well.

\newpage

# Bibliography
