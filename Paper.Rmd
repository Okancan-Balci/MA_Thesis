---
title: 'Dealing with Consumer Uncertainty: Online Product Reviews and What makes them helpful?'
bibliography: references.bib
link-citations: yes
csl: ieee.csl
---

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(AER)
library(ggfortify)
library(lubridate)

options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8,
                      echo = FALSE)
```

# Introduction

Online shopping has becoming more and more prevalent in average consumer's daily life. Electronic shopping brought many advantages both for sellers and buyers. One of the positive perks is Online Customer Reviews (OCR). Online Customer Reviews come in the form of electronic word-of-mouth @dellarocas2003 and can be defined as "*peer-generated product evaluations posted on company or third party websites*" @mudambi2010. OCR have an effect on product sales as well as consumer decision making [@duan2008; @lee2011]. \###(need different citations here!!!)\###

OCR help customers by decreasing their search costs. Customers try to get information on the product they are going to buy so that they can decrease the uncertainty about that particular product. This information acquiring process is costly. Furthermore, this information gathering process might be more or less costly depending on the product type, namely Search and Experience goods @10.2307/1830691 . Search goods, by nature, have traits that can be observed objectively before buying (GHz of a CPU of a Laptop). On the other hand, quality of Experience goods can't be evaluated before buying and evaluations could be very subjective (One needs to watch a movie in order to understand if it is a good movie or not.).

However, the increased rate of electronic commerce activity also created an abundance of product reviews on the internet. The vast amount of product reviews might increase the information overload on the customer [@sun2019; @fan2021]. Unfortunately, this information overload can make the search process even costlier because it is really time consuming to evaluate thousand of customer reviews. Additionally, not all reviews have the same informativeness level @pan2011. Therefore, answering the question of "what makes a helpful review?" is very important because reading more and more reviews can only increase the search cost. To make the understanding of a "helpful review" more clear we can define them as "*a peer-generated product evaluation that facilitates the consumer's purchase decision process*" @mudambi2010.

Since OCR don't share the same level of informativeness and usefulness @pan2011, most of the electronic retail websites implemented a very well known "helpfulness" feature to their customer review systems in order to help their customers navigate through thousand of reviews. On some websites, this feature is implemented in a way that helpful / unhelpful buttons are present and on some other websites, there is only a helpful button. In this paper, we acquired data from websites which support helpful / unhelpful system since it was found that helpfulness as a percentage (helpful votes over total votes) creates more powerful results than considering only helpful votes as a count variable @ahmad2015.

The research on perceived helpfulness is still growing. In Social and Behavioral Sciences, it is common to have conflicting papers and results in the literature @hunter1991. In this case, the study on perceived helpfulness is not an exception. Because of this reason, We think more empirical studies on perceived helpfulness are required. This paper aims to add one more perspective to growing literature of customer reviews and review helpfulness.

-   Briefly talk about other research papers and present the data.

-   Briefly talk about the results.

-   Outline the paper

# Literature Review

-   mention moderating effects of product type for every variable

The research on Online Customer Reviews could be considered to have two main branches. One branch is heavily marketing oriented and tries to understand the effects of OCR on sales [@ghose2011; @ögüt2012]. The other branch is concerned with rather behavioral or psychological outcomes and focuses on perceived helpfulness only. Of course, the boundary between these two branches are somewhat blurry and same independent variables were used to explain both helpfulness and sales. Furthermore, @topaloglu2019 found that helpfulness have a moderating effect on sales. Many other papers used perceived helpfulness to explain sales as well [@ghose2011; @lee2018]. Thus, these two branches are inter-connected with each other. This paper tries to understand behavioral aspect.

Many independent variables and their relationship between perceived helpfulness was examined by researchers. Hong et al. (2017) @hong2017 groups these independent variables into two chunks. The first group is reviewer-related predictors. Variables within this group tries to understand the relationship between reviewer status and perceived helpfulness(reviewer expertise, number of past reviews etc.). [@sun2019; @ghose2011]. The second group tries to understand the review-related relationships such as review length, review extremity, discrete review emotion etc [@mudambi2010; @ghose2011; @ren2019; @eslami2018]. Ghose et al. (2011) @ghose2011 showed that review-related predictors have stronger prediction power than reviewer-related ones. The choice of the variables is of course heavily dependent on the data itself. In this paper, we only examined review-related predictors.

Review Length(Depth) could be considered as one of the most important independent variable explaining perceived helpfulness. It might be implemented in different ways but for the majority of the studies it is the word count in a given review. For most of the researchers, longer reviews contain more information about the product. As the potential buyer gets more information from a single review, he decreases his search costs with minimum effort [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, other group of researchers found out that there was a threshold on review length, meaning longer reviews might have useless information in them[@schindler2012; @huang2015]. Another group indicates the word count of the review shows diminishing returns to helpfulness, supporting the threshold idea [@baek2012; @kuan2015; @eslami2018].

Review Rating (Review Extremity) is another important identifier of perceived helpfulness. This is usually the star rating on a 1-5 or 1-10 scale and resembles the evaluation of the reviewer for a given product. It's effect on perceived helpfulness is rather divergent in the literature. A number of researchers found out that there is a non-linear relationship between review extremity and perceived helpfulness. Specifically, a U-Shaped function which means that reviews that are on the extremes (either very negative or very positive) are more helpful than neutral reviews[@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. In other studies, linear positive relationship was found. In other words, positively rated reviews are more useful [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020]. There are also results that support negative linear relationship[@chua2014] and no effect at all[@hong2017].

Review Age is an indicator of how long the time passed after a review published on a web site. On some papers the opposite of Review Age, Review Freshness could be employed as well. Different effects on perceived helpfulness were also observed by researchers. Some researchers discussed that a timely review is very important at conveying the true information about a certain product [@liu2014; @li2019; @zhou2019]. Others discussed that some web sites promotes newer reviews and because of that more recent reviews are more helpful [@lee2014; @choi2020; @wu2021].

-   mention negative positive words here

# Theoretical Framework

## Search and Experience Goods Classification

-   extend this later and try to connect to the methodology

Nelson (1970) [@10.2307/1830691] differentiated goods based on their search and experience attributes [@nelson1974economic; @nelson1974; @nelson1981]. Search Attributes are the qualities that can be assessed before purchase. On the contrary, Experience Attributes can only be assessed by using the product itself, in other words evaluation is only possible after purchase. A single good can have both search and experience attributes. Depending on its dominant attributes the good can be classified as Search or Experience. A product's Total Cost consists of both Search Cost and Product Cost. This Search Cost is higher for Experience Goods, in other words, information search is costlier for Experience Goods than Search Goods.

To explain it in a more generalized way, let's also define what "search" is. Nelson (1970) [@10.2307/1830691] defines search as "The most obvious procedure available to the consumer in obtaining information about price or quality is search". People will always prefer sampling or trial use given the fact that the cost of the sampling or trial is lower than the cost of the search. This is why search is costlier for Experience Goods. For instance, getting information about a car (test drive, guides on the web etc.) is less costly than actually buying it. Even though a car has experience attributes, with this logic it is considered as a search good @klein1998.

-   might explain why negative reviews arent that helpful for movies

## Negativity Bias Theorem and Loss Aversion

-   needs to be connected to the method

Humans tend to care about losses more than same amount of gains. Losing 100\$ is psychologically much more impactful than finding 100\$. This behavior is best explained by "loss aversion" from Behavioral Economics. [@kahneman1979; @schindler2017]. Loss Aversion has a strong connection to Negativity Bias Theorem since negative cues might be used as an early warning against potential losses @herr1991.

Negativity Bias Theorem has four main aspects, namely, higher negative potency -- Negative signals have more impact on people than positive ones. --, steeper negative gradients -- Negativity of negatives events grows faster in space and time than positive ones. --, higher negative dominance -- A combination of equally negative and positive cues results in more negative impressions. --, and various negative differentiations -- Negative cues produce more diverse individual responses than positive ones. --[@rozin2001; @sen2007; @eslami2018].

-   Social Learning and Herding Effect

# Research Methodology

## Data Collection

The data was scraped with a custom web scraper which was constructed with Selenium Framework for Pyhton 3 @10.5555/1593511. The scraper got the customer reviews from IMDB and BestBuy. The former is the biggest platform for Movies and TV Shows, the latter is one of the biggest electronic retailers in United States of America. Both websites host a great amount of online reviews. After the data was collected it was further transformed and analyzed with the help of R Programming Language [@RCore; @Tidyverse; @Tidytext].

Online Customer Reviews from IMDB were collected as an experience goods data since movies can be considered as pure-experience goods and their search attributes are very limited. It is nearly impossible to evaluate a movie before watching it. Furthermore, evaluations for a movie can both have subjective (personal attachment to the protagonist, atmosphere of the movie etc.) and objective perspectives (writing, production quality etc.). There are also numerous studies that analyzed movies as experience goods[@10.2307/1830691; @liu2014; @ghose2006; @baek2012; @lee2016].

BestBuy is a very big electronics retailer and because of this the website offers a sizable number of OCR for search goods. Contrary to experience goods, search goods usually have more objectively quantifiable traits. For example, resolution or screen size of a TV, RAM or GPU of a Laptop, storage or battery lifetime of a Smart Phone. Additionally, in the literature, TVs, Laptops and Phones were used as search goods [@mudambi2010; @sun2019; @baek2012; @ren2019]. Therefore, we collected search goods data from BestBuy.

-   mention that channels can change the way we perceive search and experience goods.

From IMDB reviews of 22 movies were collected as experience goods. In total 22 movies account for 31,863 reviews. From BestBuy, total of 383,357 reviews were collected as search goods. Specifically, 201,113 reviews for TVs, 130,270 for Laptops and 51,974 for Phones. However, in the data there are many reviews that were never voted. To make sure that we are analyzing reviews that were actually read by people, we set a threshold for Total Reviews as five. In other words, we removed reviews that have less than 5 Total Votes [@choi2020; @baek2012]. At the very end, we were left with 15,024 observations for experience goods and 8,042 observations for search goods. It should be noted that `r round(15024 / 31863, digits = 2) * 100` percent of the reviews had total votes of 5 or greater for experience goods, whereas only `r round(8042 / 383357, digits = 2)* 100` percent of the search goods were voted at least 5 times.

It should be noted that in IMDB data there were cases (20 observations to be exact) where the same reviews were written by different user names. We removed all duplicates along with original reviews because the reviews had different helpfulness ratios.

## Variables and EDA

```{r Data_Prep_Experience}
# Experience Goods

## Read the Data

files <- dir()

files <- files[str_detect(files, "movie_.+")]

movies_data <- map_dfr(files, read_csv, show_col_types = FALSE)

## Removing reviews without Ratings
movies_data <- movies_data %>% 
  filter(!is.na(Rating)) 

## Creating Helpul and Total Votes Variables 
movies_data <- movies_data %>% 
  separate(Rating, into = c("Rating", "Total"), sep = "/", ) %>% 
  select(-Total) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Rating = as.double(Rating)) %>% 
  mutate(Helpfulness = Helpful_Vote / Total_Vote) %>% 
  mutate(Rating = Rating/2)

## Removing All the Duplicated Reviews including the original since same reviews were written by differend user names
movies_data <- movies_data %>% 
  mutate(Rev_dup = duplicated(Review) | duplicated(Review, fromLast = TRUE)) %>% 
  filter(Rev_dup == 0)

## Words
movies_data <- movies_data %>% 
  mutate(Doc_ID = row_number())

## Word Count
movies_agg <- movies_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

movies_data <- movies_data %>% 
  inner_join(movies_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- movies_data %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
movies_data <- movies_data %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = ~ifelse(is.na(.), 0, .) ))

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

movies_data <- movies_data %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))

## Freshness (Timeline)
movies_data <- movies_data %>% 
  mutate(Date = dmy(Date, locale = "english"))

first_dates <- movies_data %>% 
  group_by(Movie) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Movie, Date) %>% 
  rename(First_Date = Date)

movies_data <- full_join(movies_data, first_dates, by = "Movie") %>% 
  mutate(Freshness = as.integer(Date - First_Date))

```

```{r Data_Prep_Search}

# Search Goods
## Phone Data
phones_data <- read_csv("all_phone_reviews_15_03_22.csv", show_col_types = FALSE)

phones_data <- phones_data %>% 
  mutate(Product_Type = "Phone")

## Laptop Data
files <- dir()
files <- files[str_detect(dir(), "all_laptop_reviews.+")]
laptop_data <- map_dfr(files, read_csv, show_col_types = FALSE)
### Removing replicates that I couldn't detect earlier
laptop_data <- laptop_data %>% 
  filter(!Product_Name %in% c(
    'Samsung - Galaxy Book Pro 360 15.6" AMOLED Touch-Screen Laptop - Intel Evo Platform Core i7 - 16GB Memory - 1TB SSD - Mystic Navy',
    'ASUS - 14.0" Laptop - Intel Celeron N4020 - 4GB Memory - 64GB eMMC - Star Black - Star Black',
    'ASUS - 14" Chromebook - Intel Celeron N3350 - 4GB Memory - 32GB eMMC - Silver',
    'Samsung - Galaxy 13.3" 4K Ultra HD Touch-Screen Chromebook - Intel Core i5 - 8GB Memory - 256GB SSD - Mercury Gray'
  )) %>% 
  mutate(Product_Type = "Laptop")

## TV Data
tvs_data <- read_csv("all_tv_reviews_16_03_22.csv", show_col_types = FALSE)
tvs_data <- tvs_data %>% 
  mutate(Product_Type = "TV")

## Merging All 
search_goods <- phones_data %>% 
  bind_rows(tvs_data, laptop_data)

## Creating Total Votes variable Filtering Total Votes below 5

search_goods <- search_goods %>% 
  mutate(Total_Vote = Helpful + Unhelpful) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Helpfulness = Helpful / Total_Vote) %>% 
  mutate(is_Exp = 0)

search_goods <- search_goods %>% 
  mutate(Dupped = duplicated(Review)) %>% 
  filter(Dupped == 0)

## Word Counts

search_goods <- search_goods %>% 
  mutate(Doc_ID = row_number()) 

words_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

search_goods <- search_goods %>% 
  inner_join(words_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
search_goods <- search_goods %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = ~ifelse(is.na(.), 0, .) ))


search_goods <- search_goods %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))


## Freshness (Timeline)

search_goods <- search_goods %>% 
  mutate(Date = as_date(mdy_hm(Date, locale = "english")))

first_dates <- search_goods %>% 
  group_by(Product_Name) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Product_Name, Date) %>% 
  rename(First_Date = Date)

search_goods <- search_goods %>% 
  full_join(first_dates, by = "Product_Name") %>% 
  mutate(Freshness = as.integer(Date - First_Date))
```

### Independent Variable

## Method

# Results

# Discussion and Conclusion

# References
