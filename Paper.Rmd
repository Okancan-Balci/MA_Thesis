---
title: 'Dealing with Consumer Uncertainty: Online Product Reviews and What makes them helpful?'
bibliography: references.bib
link-citations: yes
csl: ieee.csl
output:
  bookdown::pdf_document2: 
    fig.caption: yes
    includes:
      in_header: non-float-fig.tex
    toc_depth: 4
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(AER)
library(ggfortify)
library(lubridate)
library(bookdown)
library(patchwork)

theme_set(theme_bw(base_size = 12))

options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 7,
                      echo = FALSE)
```

# Introduction

Online shopping has becoming more and more prevalent in average consumer's daily life. Electronic shopping brought many advantages both for sellers and buyers. One of the positive perks is Online Customer Reviews (OCR). Online Customer Reviews come in the form of electronic word-of-mouth @dellarocas2003 and can be defined as "*peer-generated product evaluations posted on company or third party websites*" @mudambi2010. OCR have an effect on product sales as well as consumer decision making [@duan2008; @lee2011]. \###(need different citations here!!!)\###

OCR help customers by decreasing their search costs. Customers try to get information on the product they are going to buy so that they can decrease the uncertainty about that particular product. This information acquiring process is costly. Furthermore, this information gathering process might be more or less costly depending on the product type, namely Search and Experience goods @nelson1970 . Search goods, by nature, have traits that can be observed objectively before buying (GHz of a CPU of a Laptop). On the other hand, quality of Experience goods can't be evaluated before buying and evaluations could be very subjective (One needs to watch a movie in order to understand if it is a good movie or not.).

However, the increased rate of electronic commerce activity also created an abundance of product reviews on the internet. The vast amount of product reviews might increase the information overload on the customer [@sun2019; @fan2021]. Unfortunately, this information overload can make the search process even costlier because it is really time consuming to evaluate thousand of customer reviews. Additionally, not all reviews have the same informativeness level @pan2011. Therefore, answering the question of "what makes a helpful review?" is very important because reading more and more reviews can only increase the search cost. To make the understanding of a "helpful review" more clear we can define them as "*a peer-generated product evaluation that facilitates the consumer's purchase decision process*" @mudambi2010.

Since OCR don't share the same level of informativeness and usefulness @pan2011, most of the electronic retail websites implemented a very well known "helpfulness" feature to their customer review systems in order to help their customers navigate through thousand of reviews. On some websites, this feature is implemented in a way that helpful / unhelpful buttons are present and on some other websites, there is only a helpful button. In this paper, we acquired data from websites which support helpful / unhelpful system since it was found that helpfulness as a percentage (helpful votes over total votes) creates more powerful results than considering only helpful votes as a count variable @ahmad2015.

The research on perceived helpfulness is still growing. In Social and Behavioral Sciences, it is common to have conflicting papers and results in the literature @hunter1991. In this case, the study on perceived helpfulness is not an exception. Because of this reason, We think more empirical studies on perceived helpfulness are required. This paper aims to add one more perspective to growing literature of customer reviews and review helpfulness.

-   Briefly talk about other research papers and present the data.

-   Briefly talk about the results.

-   Outline the paper

# Literature Review

-   mention moderating effects of product type for every variable

The research on Online Customer Reviews could be considered to have two main branches. One branch is heavily marketing oriented and tries to understand the effects of OCR on sales [@ghose2011; @ögüt2012]. The other branch is concerned with rather behavioral or psychological outcomes and focuses on perceived helpfulness only. Of course, the boundary between these two branches are somewhat blurry and same independent variables were used to explain both helpfulness and sales. Furthermore, @topaloglu2019 found that helpfulness have a moderating effect on sales. Many other papers used perceived helpfulness to explain sales as well [@ghose2011; @lee2018]. Thus, these two branches are inter-connected with each other. This paper tries to understand behavioral aspect.

Many independent variables and their relationship between perceived helpfulness was examined by researchers. Hong et al. (2017) @hong2017 groups these independent variables into two chunks. The first group is reviewer-related predictors. Variables within this group tries to understand the relationship between reviewer status and perceived helpfulness(reviewer expertise, number of past reviews etc.). [@sun2019; @ghose2011]. The second group tries to understand the review-related relationships such as review length, review extremity, discrete review emotion etc [@mudambi2010; @ghose2011; @ren2019; @eslami2018]. Ghose et al. (2011) @ghose2011 showed that review-related predictors have stronger prediction power than reviewer-related ones. The choice of the variables is of course heavily dependent on the data itself. In this paper, we only examined review-related predictors.

Review Length(Depth) could be considered as one of the most important independent variable explaining perceived helpfulness. It might be implemented in different ways but for the majority of the studies it is the word count in a given review. For most of the researchers, longer reviews contain more information about the product. As the potential buyer gets more information from a single review, he decreases his search costs with minimum effort [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, other group of researchers found out that there was a threshold on review length, meaning longer reviews might have useless information in them[@schindler2012; @huang2015]. Another group indicates the word count of the review shows diminishing returns to helpfulness, supporting the threshold idea [@baek2012; @kuan2015; @eslami2018].

Review Rating (Review Extremity) is another important identifier of perceived helpfulness. This is usually the star rating on a 1-5 or 1-10 scale and resembles the evaluation of the reviewer for a given product. It's effect on perceived helpfulness is rather divergent in the literature. A number of researchers found out that there is a non-linear relationship between review extremity and perceived helpfulness. Specifically, a U-Shaped function which means that reviews that are on the extremes (either very negative or very positive) are more helpful than neutral reviews[@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. In other studies, linear positive relationship was found. In other words, positively rated reviews are more useful [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020]. There are also results that support negative linear relationship[@chua2014] and no effect at all[@hong2017].

Review Age is an indicator of how long the time passed after a review published on a web site. On some papers the opposite of Review Age, Review Freshness could be employed as well. Different effects on perceived helpfulness were also observed by researchers. Some researchers discussed that a timely review is very important at conveying the true information about a certain product [@liu2014; @li2019; @zhou2019]. Others discussed that some web sites promotes newer reviews and because of that more recent reviews are more helpful [@lee2014; @choi2020; @wu2021].

-   mention negative positive words here

# Theoretical Framework

## Search and Experience Goods Classification

-   extend this later and try to connect to the methodology

Nelson (1970) [@nelson1970] differentiated goods based on their search and experience attributes [@nelson1974economic; @nelson1974; @nelson1981]. Search Attributes are the qualities that can be assessed before purchase. On the contrary, Experience Attributes can only be assessed by using the product itself, in other words evaluation is only possible after purchase. A single good can have both search and experience attributes. Depending on its dominant attributes the good can be classified as Search or Experience. A product's Total Cost consists of both Search Cost and Product Cost. This Search Cost is higher for Experience Goods, in other words, information search is costlier for Experience Goods than Search Goods.

To explain it in a more generalized way, let's also define what "search" is. Nelson (1970) [@nelson1970] defines search as "The most obvious procedure available to the consumer in obtaining information about price or quality is search". People will always prefer sampling or trial use given the fact that the cost of the sampling or trial is lower than the cost of the search. This is why search is costlier for Experience Goods. For instance, getting information about a car (test drive, guides on the web etc.) is less costly than actually buying it. Even though a car has experience attributes, with this logic it is considered as a search good @klein1998.

-   might explain why negative reviews arent that helpful for movies

## Negativity Bias Theorem and Loss Aversion

-   needs to be connected to the method

Humans tend to care about losses more than same amount of gains. Losing 100\$ is psychologically much more impactful than finding 100\$. This behavior is best explained by "loss aversion" from Behavioral Economics. [@kahneman1979; @schindler2017]. Loss Aversion has a strong connection to Negativity Bias Theorem since negative cues might be used as an early warning against potential losses @herr1991.

Negativity Bias Theorem has four main aspects, namely, higher negative potency -- Negative signals have more impact on people than positive ones. --, steeper negative gradients -- Negativity of negatives events grows faster in space and time than positive ones. --, higher negative dominance -- A combination of equally negative and positive cues results in more negative impressions. --, and various negative differentiations -- Negative cues produce more diverse individual responses than positive ones. --[@rozin2001; @sen2007; @eslami2018].

-   Social Learning and Herding Effect

# Research Methodology

## Data Collection

The data was scraped with a custom web scraper which was constructed with Selenium Framework for Pyhton 3 @python3. The scraper got the customer reviews from IMDB and BestBuy. The former is the biggest platform for Movies and TV Shows, the latter is one of the biggest electronic retailers in United States of America. Both websites host a great amount of online reviews. After the data was collected it was further transformed and analyzed with the help of R Programming Language [@RCore; @Tidyverse; @Tidytext].

Online Customer Reviews from IMDB were collected as an experience goods data since movies can be considered as pure-experience goods and their search attributes are very limited. It is nearly impossible to evaluate a movie before watching it. Furthermore, evaluations for a movie can both have subjective (personal attachment to the protagonist, atmosphere of the movie etc.) and objective perspectives (writing, production quality etc.). There are also numerous studies that analyzed movies as experience goods[@nelson1970] ; @liu2014; @ghose2006; @baek2012; @lee2016].

BestBuy is a very big electronics retailer and because of this the website offers a sizable number of OCR for search goods. Contrary to experience goods, search goods usually have more objectively quantifiable traits. For example, resolution or screen size of a TV, RAM or GPU of a Laptop, storage or battery lifetime of a Smart Phone. Additionally, in the literature, TVs, Laptops and Phones were used as search goods [@mudambi2010; @sun2019; @baek2012; @ren2019]. Therefore, we collected search goods data from BestBuy.

-   mention that channels can change the way we perceive search and experience goods.

From IMDB reviews of 22 movies were collected as experience goods. In total 22 movies account for 31,863 reviews. From BestBuy, total of 383,357 reviews were collected as search goods. Specifically, 201,113 reviews for TVs, 130,270 for Laptops and 51,974 for Phones. However, in the data there are many reviews that were never voted. To make sure that we are analyzing reviews that were actually read by people, we set a threshold for Total Reviews as five. In other words, we removed reviews that have less than 5 Total Votes [@choi2020; @baek2012]. At the very end, we were left with 15,024 observations for experience goods and 8,042 observations for search goods. It should be noted that `r round(15024 / 31863, digits = 2) * 100` percent of the reviews had total votes of 5 or greater for experience goods, whereas only `r round(8042 / 383357, digits = 2)* 100` percent of the search goods were voted at least 5 times.

It should be noted that in IMDB data there were cases (20 observations to be exact) where the same reviews were written by different user names. All duplicates along with original reviews were removed because the reviews had different helpfulness ratios.

## Exploratory Data Analysis

```{r Data_Prep_Experience}
# Experience Goods

## Read the Data

files <- dir()

files <- files[str_detect(files, "movie_.+")]

movies_data <- map_dfr(files, read_csv, show_col_types = FALSE)

## Removing reviews without Ratings
movies_data <- movies_data %>% 
  filter(!is.na(Rating)) 

## Creating Helpul and Total Votes Variables 
movies_data <- movies_data %>% 
  separate(Rating, into = c("Rating", "Total"), sep = "/", ) %>% 
  select(-Total) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Rating = as.double(Rating)) %>% 
  mutate(Helpfulness = Helpful_Vote / Total_Vote) %>% 
  mutate(Rating = Rating/2)

## Removing All the Duplicated Reviews including the original since same reviews were written by differend user names
movies_data <- movies_data %>% 
  mutate(Rev_dup = duplicated(Review) | duplicated(Review, fromLast = TRUE)) %>% 
  filter(Rev_dup == 0)

## Words
movies_data <- movies_data %>% 
  mutate(Doc_ID = row_number())

## Word Count
movies_agg <- movies_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

movies_data <- movies_data %>% 
  inner_join(movies_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- movies_data %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
movies_data <- movies_data %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = ~ifelse(is.na(.), 0, .) ))

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

movies_data <- movies_data %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))

## Freshness (Timeline)
movies_data <- movies_data %>% 
  mutate(Date = dmy(Date, locale = "english"))

first_dates <- movies_data %>% 
  group_by(Movie) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Movie, Date) %>% 
  rename(First_Date = Date)

movies_data <- full_join(movies_data, first_dates, by = "Movie") %>% 
  mutate(Freshness = as.integer(Date - First_Date))

```

```{r Data_Prep_Search}

# Search Goods
## Phone Data
phones_data <- read_csv("all_phone_reviews_15_03_22.csv", show_col_types = FALSE)

phones_data <- phones_data %>% 
  mutate(Product_Type = "Phone")

## Laptop Data
files <- dir()
files <- files[str_detect(dir(), "all_laptop_reviews.+")]
laptop_data <- map_dfr(files, read_csv, show_col_types = FALSE)
### Removing replicates that I couldn't detect earlier
laptop_data <- laptop_data %>% 
  filter(!Product_Name %in% c(
    'Samsung - Galaxy Book Pro 360 15.6" AMOLED Touch-Screen Laptop - Intel Evo Platform Core i7 - 16GB Memory - 1TB SSD - Mystic Navy',
    'ASUS - 14.0" Laptop - Intel Celeron N4020 - 4GB Memory - 64GB eMMC - Star Black - Star Black',
    'ASUS - 14" Chromebook - Intel Celeron N3350 - 4GB Memory - 32GB eMMC - Silver',
    'Samsung - Galaxy 13.3" 4K Ultra HD Touch-Screen Chromebook - Intel Core i5 - 8GB Memory - 256GB SSD - Mercury Gray'
  )) %>% 
  mutate(Product_Type = "Laptop")

## TV Data
tvs_data <- read_csv("all_tv_reviews_16_03_22.csv", show_col_types = FALSE)
tvs_data <- tvs_data %>% 
  mutate(Product_Type = "TV")

## Merging All 
search_goods <- phones_data %>% 
  bind_rows(tvs_data, laptop_data)

## Creating Total Votes variable Filtering Total Votes below 5

search_goods <- search_goods %>% 
  mutate(Total_Vote = Helpful + Unhelpful) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Helpfulness = Helpful / Total_Vote) %>% 
  mutate(is_Exp = 0)

search_goods <- search_goods %>% 
  mutate(Dupped = duplicated(Review)) %>% 
  filter(Dupped == 0)

## Word Counts

search_goods <- search_goods %>% 
  mutate(Doc_ID = row_number()) 

words_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

search_goods <- search_goods %>% 
  inner_join(words_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
search_goods <- search_goods %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = ~ifelse(is.na(.), 0, .) ))


search_goods <- search_goods %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))


## Freshness (Timeline)

search_goods <- search_goods %>% 
  mutate(Date = as_date(mdy_hm(Date, locale = "english")))

first_dates <- search_goods %>% 
  group_by(Product_Name) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Product_Name, Date) %>% 
  rename(First_Date = Date)

search_goods <- search_goods %>% 
  full_join(first_dates, by = "Product_Name") %>% 
  mutate(Freshness = as.integer(Date - First_Date))
```

In this section both dependent and independent variables are going to be introduced. The goal of this section is not "confirmatory" whatsoever. As a researcher, I believe sharing the data that I used in the study transparently increases the research's reproducibility value.

As an outline, firstly the dependent variable is going to be introduced in a univariate manner and then independent variables are going to come into the picture both in univariate and bivariate way.

### Dependent Variable

$$
Perceived\ Helpfulness = \frac{Helpful\ Votes}{Total\ Votes}
$$

The ratio of helpfulness is derived from the formula above. Below on Figure \@ref(fig:IV-Hist-Search) the distribution of the helpfulness for search goods can be observed.

```{r IV-Hist-Search, fig.cap="Distribution of the Helpfulness of Search Goods"}

search_goods %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = NULL)
  
```

As it can be seen the distribution is rather skewed. Looks like in the electronics case, readers find some reviews very helpful. In fact, in the data there is one review which had over 170 helpful votes with zero unhelpful votes. On the other hand, distribution for experience goods on Figure \@ref(fig:IV-Hist-Exp) presents a different picture. This time helpfulness ratio is sort of normally distributed. There aren't very helpful reviews. The most helpful review interval seems to be lying between 70% to 80%.

```{r IV-Hist-Exp, fig.cap="Distribution of the Helpfulness of Experience Goods"}
movies_data %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = NULL)
```

Hong et al. (2014) [@hong2014; @sun2019] states that consumers likely to form homogeneous opinions about search goods. On the contrary, the same paper suggests that consumers' opinions are more heterogeneous on experience goods. Intuitively, what these histograms show is the same. For instance, positive or negative attributes of search goods can be expressed in a limited number of ways. The storage space of a phone might be small or CPU of a laptop might not be fast as expected. Thus, one good review is be enough to reveal the quality of a product. I think this is why we see such a big spike at 100% on Figure \@ref(fig:IV-Hist-Search). In the movies (experience good) case on Figure \@ref(fig:IV-Hist-Exp), we see that readers usually disagree with each other because the distribution of the perceived helpfulness distributed around 50%. This is because evaluation process of movies is rather subjective. One person might not like Spider-Man: Homecoming and another would love it.

-   you can elaborate a little bit more about how movies cant be evaluated objectively

### Independent Variables

#### Review Rating (Review Extremity)

Review Rating is the star rating that was given by the reviewer as an evaluation of a product.

For both search and experience goods the bi-modality of the distributions can be observed on Figure \@ref(fig:Search-Rating) (A) and Figure \@ref(fig:Experience-Rating) (A) respectively. Hu et al. @hu2006 also pointed out a similar distribution concerning Online Product Reviews. This might mean that consumers usually tend to write product reviews when they are extremely happy or unhappy with the product.

Because the data was taken from two different websites (BestBuy and IMDB), there are two different scales. For IMDB that is 1 to 10 and for BestBuy the scale is between 1 and 5. To make the interpretation of the regression coefficients uniform, IMDB's scale was divided by 2.

On Figure \@ref(fig:Search-Rating) (B) and Figure \@ref(fig:Experience-Rating) (B) the bivariate relationship between perceived helpfulness and review extremity can be seen. Again, as a researcher, I must emphasize that this analysis is not confirmatory. On figures, we can observe "U" type of relationship for both Search and Experience goods, though the non-linearity seems to be stronger with Search goods. This might suggest that on the average extreme reviews might be more helpful than moderate reviews [@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. On the other hand, it is observable that positive reviews on the average are more helpful than the rest. This might indicate that there is a linear positive relationship [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020].

```{r Search-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Search Goods.", fig.height=7}
p1 <- search_goods %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  labs(y = "Count", x = NULL)

p2 <- search_goods %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(method = "loess", color = "steelblue")+
  labs(x = "Review Rating")

plots <-  p1 / p2

plots + plot_annotation(tag_levels = "A")
```

```{r Experience-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Experience Goods.", fig.height=7}
p1 <- movies_data %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  scale_x_continuous(n.breaks = 10)+
  labs(y = "Count", x = NULL)

p2 <- movies_data %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 10)+
  labs(x = "Review Rating")

plots <-  p1 / p2

plots + plot_annotation(tag_levels = "A")
```

#### Review Length (Review Depth)

Review Length is the number of the words in a given review.

The exponential distribution of the number of words can be observed for both good types on Figure \@ref(fig:Search-Length) (A) and Figure \@ref(fig:Experience-Length) (A). This means that reviews become less prevalent as they get longer. This might suggest logarithmic transformation which can increase the model fit. For the transformed distributions see Figure \@ref(fig:Search-Length) (B) and Figure \@ref(fig:Experience-Length) (B).

-   you might add confidence interval for mean on plots.

The bivariate relationship between the number of words and Helpfulness seems to be varying for different product types. For Search goods on Figure \@ref(fig:Search-Length) (C) there is a clear positive relationship [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, this positive relationship appears to stop at a threshold[@schindler2012; @huang2015; @baek2012; @kuan2015; @eslami2018]. With this in mind, logarithmic transformation might make more sense (see Figure \@ref(fig:Search-Length) (D)) for modelling purposes. For Experience goods, on the other hand, there seems to be a weak negative relationship or no relationship at all (see Figure \@ref(fig:Experience-Length) (C) and (D)).

-   maybe add more framework for experience goods and no relationship

```{r Search-Length, fig.height=8, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Number of the Words")

p1_log <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Number of the Words)")

p2 <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Number of the Words")

p2_log <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Number of the Words)")

plots <-  p1 / p1_log / p2 / p2_log

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Length, fig.height=8, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Number of the Words")

p1_log <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Number of the Words)")

p2 <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Number of the Words")

p2_log <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Number of the Words)")

plots <-  p1 / p1_log / p2 / p2_log

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Freshness of A Review

Review Freshness indicates how recent a review is. To engineer this variable following steps were taken:

1.  The first review date for a particular brand and model of a product (movie, phone, laptop, tv) was taken.
2.  The first dates for particular products then subtracted from other reviews' dates under the respective brand and model resulting an integer variable representing days.
3.  Bigger numbers represent more recent reviews.

Distributions of both product types resemble exponential distribution (see Figure \@ref(fig:Search-Freshness) (A) and Figure \@ref(fig:Experience-Freshness) (A)). Again a logarithmic transformation might be useful here (see Figure \@ref(fig:Search-Freshness) (B) and Figure \@ref(fig:Experience-Freshness) (B)) . Interestingly, even after 3 years from the release date some products are still getting reviewed from customers.

On the scatter plots a weak relationship between perceived helpfulness and review freshness could be observed. The smoother line suggests both positive [@lee2014; @choi2020; @wu2021] and negative [@liu2014; @li2019; @zhou2019] linear relationship between two variables. This non-linearity may explain different findings in the literature.

```{r Search-Freshness, fig.height=8, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Freshness of Review")

p1_log <- search_goods %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Freshness of Review + 1)")

p2 <- search_goods %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Freshness of Review")

p2_log <- search_goods %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- p1 / p1_log / p2 / p2_log

plots + plot_annotation(
  tag_levels = "A"
)

```

```{r Experience-Freshness, fig.height=8, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Freshness of Review")

p1_log <- movies_data %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Freshness of Review + 1)")

p2 <- movies_data %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Freshness of Review")

p2_log <- movies_data %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- p1 / p1_log / p2 / p2_log

plots + plot_annotation(
  tag_levels = "A"
)

```

#### Proportion of Negative Words

Negative words were derived by using NRC Word-Emotion Association Lexicon @Mohammad13. A lexicon is basically a dictionary of words. These words might have similar emotions or sentiments. In this case, negative words lexicon was used in order to engineer this variable.

$$
Negative\ Word \% = \frac{Number\ of\ Matched\ Negative\ Word}{Total\ Words\ in\ Review} 
$$

These steps were taken in order to create this variable:

1.  Words in a review were tokenized.
2.  Tokenized words were matched with the lexicon.
3.  Matches were counted and divided by the total number of words with the respective review then multiplied by 100 in order to make regression results more interpretable.
4.  Logarithmic transformation was applied in order to scale the variable in a better way.

On Figure \@ref(fig:Search-Neg) (A) and Figure \@ref(fig:Experience-Neg) (A) the distributions of the variable for both good types can be seen. On plots, x-axis labels were given in the logarithm of base 10 so 1.00 on the x-axis implies that 10% of the review was negative words. It should be noted that `r round(mean(search_goods$negative == 0), digits = 2)` percent of the reviews for search goods don't have any negative words in them. For experience goods this number is `r round(mean(movies_data$negative == 0), digits = 2)` percent.

-   comment on scatterplot

```{r Search-Neg, fig.height=7, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Search Goods"}
p1 <- search_goods %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = NULL)

p2 <- search_goods %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 / p2

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Neg, fig.height=7, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Experience Goods"}
p1 <- movies_data %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = NULL)

p2 <- movies_data %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 / p2

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Proportion of Positive Words

## Hypothesis Development and Method

# Results

# Discussion and Conclusion

# References
