---
title: 'Dealing with Consumer Uncertainty: Online Product Reviews and What makes them helpful?'
bibliography: references.bib
link-citations: yes
output:
  bookdown::pdf_document2: 
    fig.caption: yes
    includes:
      in_header: non-float-fig.tex
    toc_depth: 4
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(AER)
library(ggfortify)
library(lubridate)
library(bookdown)
library(patchwork)
library(GGally)
library(stargazer)

theme_set(theme_bw(base_size = 12))

options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 8,
                      fig.height = 4,
                      echo = FALSE)
```

# Introduction

Online shopping has becoming more and more prevalent in average consumer's daily life. Electronic shopping brought many advantages both for sellers and buyers. One of the positive perks is Online Customer Reviews (OCR). Online Customer Reviews come in the form of electronic word-of-mouth [@dellarocas2003] and can be defined as "*peer-generated product evaluations posted on company or third party websites*" [@mudambi2010]. OCR have an effect on product sales as well as consumer decision making [@duan2008; @lee2011; @dellarocas2007].

OCR help customers by decreasing their search costs. Customers try to get information on the product they are going to buy so that they can decrease the uncertainty about that particular product. This information acquiring process is costly. Furthermore, this information gathering process might be more or less costly depending on the product type, namely Search and Experience goods [@nelson1970] . Search goods, by nature, have traits that can be observed objectively before buying (GHz of a CPU of a Laptop). On the other hand, quality of Experience goods can't be evaluated before buying and evaluations could be very subjective (One needs to watch a movie in order to understand if it is a good movie or not.).

However, the increased rate of electronic commerce activity also created an abundance of product reviews on the internet. The vast amount of product reviews might increase the information overload on the customer [@sun2019; @fan2021]. Unfortunately, this information overload can make the search process even costlier because it is really time consuming to evaluate thousand of customer reviews. Additionally, not all reviews have the same informativeness level [@pan2011]. Therefore, answering the question of "what makes a helpful review?" is very important because reading more and more reviews can only increase the search cost. To make the understanding of a "helpful review" more clear it can be defined as "*a peer-generated product evaluation that facilitates the consumer's purchase decision process*" [@mudambi2010].

Since OCR don't share the same level of informativeness and usefulness [@pan2011], most of the electronic retail websites implemented a very well known "helpfulness" feature to their customer review systems in order to help their customers navigate through thousand of reviews. On some websites, this feature is implemented in a way that helpful / unhelpful buttons are present and on some other websites, there is only a helpful button. In this study, the data was acquired from websites which support helpful / unhelpful system since it was found that helpfulness as a percentage (helpful votes over total votes) creates more powerful results than considering only helpful votes as a count variable [@ahmad2015].

The research on perceived helpfulness is still growing. In Social and Behavioral Sciences, it is common to have conflicting papers and results in the literature [@hunter1991]. In this case, the study on perceived helpfulness is not an exception. Because of this reason, more empirical studies on perceived helpfulness are required. This paper aims to add one more perspective to the growing literature of customer reviews and review helpfulness.

This study uses secondary data. The reviews were collected from two websites. Namely, IMDB and BestBuy. The data was scrapped with Selenium Webdriver for Python [@python3]. Over 400,000 reviews and their meta data were gathered initially. After the data transformation process the final sample size was over 23,000. Then this data was subjected to empirical tests. It was found out that consumers usually find extreme reviews more helpful than neutral ones. Furthermore, consumers prefer longer reviews when reviews are rated negatively. On the contrast, positive reviews with fewer words were deemed more helpful by readers on the average. These effects were further moderated by product types, namely, Search and Experience goods.

The plan of this study is outlined as the following. In the next section, the literature on the subject of Review Helpfulness is going to be presented. In the third section, the related theories which would make the understanding of the consumer behavior and review helpfulness will be laid out. Next, the data and the research methodology are going to be introduced. The results will be shared in the fifth section. In the last section the results are going to be discussed and the study is going to be concluded.

# Literature Review

The research on Online Customer Reviews could be considered to have two main branches. One branch is heavily marketing oriented and tries to understand the effects of OCR on sales [@ghose2011; @ögüt2012]. The other branch is concerned with rather behavioral or psychological outcomes and focuses on perceived helpfulness only. The boundary between these two branches are somewhat blurry and the same independent variables were used to explain both helpfulness and sales. Furthermore, @topaloglu2019 found that helpfulness have a moderating effect on sales. Many other papers used perceived helpfulness to explain sales as well [@ghose2011; @lee2018]. Thus, these two branches are inter-connected with each other. This paper tries to understand behavioral aspect of the review helpfulness rather than its effects on sales.

Many independent variables and their relationship between perceived helpfulness were examined by researchers. @hong2017 groups these independent variables into two chunks. The first group is reviewer-related predictors. Variables within this group try to understand the relationship between reviewer status and perceived helpfulness(reviewer expertise, number of past reviews etc.). [@sun2019; @ghose2011]. The second group tries to understand the review-related relationships such as review length, review extremity, discrete review emotion etc [@mudambi2010; @ghose2011; @ren2019; @eslami2018]. @ghose2011 showed that review-related predictors have stronger prediction power than reviewer-related ones. The choice of the variables is heavily dependent on the data itself. In my paper, only review-related predictors were examined because the data which is available for this study only allows for review-related variables.

Review Length (Depth) could be considered as one of the most important independent variable explaining perceived helpfulness. It might be implemented in different ways but for the majority of the studies it is the word count in a given review. For most of the researchers, longer reviews contain more information about the product. As the potential buyer gets more information from a single review, he decreases his search costs with minimum effort [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, other group of researchers found out that there was a threshold on review length, meaning longer reviews might have useless information in them[@schindler2012; @huang2015]. Another group indicates the word count of the review shows diminishing returns to helpfulness, supporting the threshold idea [@baek2012; @kuan2015; @eslami2018].

Review Rating (Review Extremity) is another important identifier of perceived helpfulness. This is usually the star rating on a 1-5 or 1-10 scale and resembles the evaluation of the reviewer for a given product. Its effect on perceived helpfulness is rather divergent in the literature. A number of researchers found out that there is a non-linear relationship between review extremity and perceived helpfulness. Specifically, a U-Shaped function which means that reviews that are on the extremes (either very negative or very positive) are more helpful than neutral reviews[@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. In other studies, linear positive relationship was found. In other words, positively rated reviews are more useful [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020]. There are also results that support negative linear relationship[@chua2014] and no effect at all[@hong2017].

Review Age is an indicator of how long the time passed after a review published on a web site. On some papers the opposite of Review Age, Review Freshness (Recency) could be employed as well. Different effects on perceived helpfulness were also observed by researchers. Some researchers discussed that a timely review is very important at conveying the true information about a certain product [@liu2014; @li2019; @zhou2019]. Others discussed that some web sites promotes newer reviews and because of that more recent reviews are more helpful [@lee2014; @choi2020; @wu2021].

The counts of negative or positive words out of total word count (the proportion of negative and positive words) were employed by a handful of studies [@baek2012; @kuan2015]. These metrics can measure the sentiment of the review itself. They are usually different than Review Extremity. In most cases Review Extremity and Review Sentiment (Valence) don't match [@zhang2011]. @baek2012 and @kuan2015 discovered a positive relationship between the proportion of negative words and perceived helpfulness. The relationship between the proportion of positive words and perceived helpfulness was found to be negative [@kuan2015].

The classification of Experience and Search goods plays a moderating role in the literature [@mudambi2010; @sun2019; @baek2012; @lee2016]. Some of these studies showed that Review Length has a greater effect for search products than experience products [@mudambi2010; @baek2012; @lee2016]. These findings suggest that search goods can be evaluated in a more objective way than experience goods. There is also moderating effect of product type on Review Extremity. @mudambi2010 showed that moderate reviews are more helpful for experience products than search products. @baek2012 and @lee2016 had the results which indicated that more extreme reviews (negative or positive) is much more helpful for search goods than experience goods.

Many different types of research methodologies were employed in the literature of the Review Helpfulness. To my knowledge the most common empirical analysis method that was used in the literature was Tobit Regression because of the limited and censored nature of the Reviews Helpfulness [@mudambi2010; @korfiatis2012; @huang2015; @choi2020; @ren2019; @chua2014]. Some of these studies also shared Ordinary Least Squares results alongside with the Tobit Regression results for the purpose of benchmarking [@mudambi2010; @kuan2015; @choi2020]. Ordinary Least Squares were also used as a main empirical methodology in some papers [@liu2014; @ghose2006; @lee2016]. Furthermore, Ordinary Least Squares was employed in a hierarchical manner as well [@baek2012]. Some other studies employed Multi-Level Regression models in order to deal with potential biases [@ghose2011; @wu2021; @kuan2015]. In a rare case Ordinal Logistic Regression was one of the methods that was used to explain the determinants of the review helpfulness [@cao2011]. Predictive methods such as Neural Networks were used to understand the components of a helpful review as well [@lee2014]. One paper attacked the problem of the review helpfulness with three methods, namely, Partial Least Squares - Structural Equation Modelling, ANOVA Analysis and Artificial Neural Networks [@eslami2018].

Any website which offers online reviews for products could be used as a data source (Yelp, Booking, BestBuy, eBay) in the literature of review helpfulness but the majority of the aforementioned studies used the data from either websites Amazon or Internet Movie Database (IMDB) because these websites offer relatively easier ways to get the needed data. They have their respective APIs and on top of that, it is relatively easier to scrape both websites. Additionally, both websites offer product reviews for a numerous different types of products. Amazon is a huge e-retailer and offers a wide range of items. IMDB is a home for both critic and user reviews of Movies and TV Shows. In some other studies local equivalents of Amazon was used as well. [@sun2019]. @cao2011 used data from CNET Download.com which is a software distributor.

There might be variety reasons why there are some conflicting results in the literature. First of all, many different data sources were used in the studies. Even in the same data source, for instance Amazon, there exist a wide range of products with different brands. Different sub-groups of products can receive different feedbacks. This might increase the potential noise in the data, leading conflicting results. Secondly, many different research designs and research methodologies were employed. In these different research designs different independent and control variables were used because the data at the hand may allow for different sets of variables. Lastly, there is no established theory of review helpfulness. This means that studies are rather unorganized and researchers are usually inclining towards their own domain of expertise when it comes to explaining review helpfulness.

# Theoretical Framework

## Search and Experience Goods Classification

Relaxing the assumption that consumers have perfect information about the products on the market raises the problem of information acquisition for consumers. Getting the complete information about products is important because economic agents want to maximize their utility by finding the best good on the market. The complete information is the information about both the prices and quality levels of the goods which are available on the market. Acquiring the information of the quality of a product is harder than getting the information about its price. To explain, a consumer can check the price list of laptops and have an idea about maximum and minimum prices but getting the information about a laptop's technical components or a brand's support for its products is a lot more effort taking and costly.

In order to solve this problem @nelson1970 differentiated goods based on their search and experience attributes [@nelson1974economic; @nelson1974; @nelson1981]. Search Attributes are the qualities that can be assessed before purchase. On the contrary, Experience Attributes can only be assessed by using the product itself, in other words, evaluation is only possible after purchase. A single good can have both search and experience attributes. Depending on its dominant attributes the good can be classified as Search or Experience. @darby1973 extended Search and Experience goods framework with Credence goods where consumers can't evaluate the good even after the consumption. Though, Credence goods won't be discussed any further in this study.

Consumers can choose from two different strategies when they want to acquire more information about products. Consumers "search" in order to get information about a product assuming that they perfectly know where they can obtain the knowledge. @nelson1970 defines the search process as *"The most obvious procedure available to the consumer in obtaining information about price or quality is search"*. The search process is complete when consumers inspect the option and that inspection happens before the purchase. Contrary to the search process, consumer may choose to "experience" the product. The consumer follows this strategy when the search for information is either inappropriate, inefficient or costly compared to just experiencing the good. This is true for cheap goods especially. For instance, in order to find out the best chocolate in the market it's best for the consumer to buy every other brand available and taste them. Thus, consumers will always prefer sampling or trial use given the fact that the cost of the sampling or trial is lower than the cost of the search. This is why search is costlier for Experience goods. Therefore, the kind of strategy that a consumer chooses depends on the product's Search or Experience attributes and how dominant these attributes are.

@nelson1970 also states that consumers can use other consumers' experiences in their search processes. The information about a certain good is usually acquired in the form of "word-of"mouth" in this kind of search process. This type of search is viable when getting the information about a certain product is much more costly by "experience" compared to by "search". @klein1998 discusses that experience sharing becomes easier with the increasing usage of the internet. Because experiences about products can be shared much more easily on the internet, consumers' pre-purchase behavior might change accordingly. The reason of this is that internet decreases search costs considerably. Furthermore, the potential consumer may engage in "vicarious learning" where he/she gets indirect usage experience through the experiences of other consumers in the search process [@murray1991].

Ultimately, to put the theory in a more generalized way, a product's Total Cost consists of both Search Cost and Product Cost. This Search Cost is higher for Experience goods, in other words, information search is costlier for Experience goods than Search goods. In the consumer's perspective, to make the search viable, the marginal benefit of an additional search must be grater than the marginal cost of the additional search. The increasing use of internet decreases the search costs. Because of the decreased search costs, the consumer may undertake more search processes and this might change how people perceive good types, in other words, internet and virtual experience sharing can convert some Experience goods to Search goods [@klein1998].

## Negativity Bias Theorem and Loss Aversion

-   should be extended a little bit more

Humans tend to care about losses more than same amount of gains. Losing 100\$ is psychologically much more impactful than finding 100\$. This behavior is best explained in "loss aversion" from behavioral economics. [@kahneman1979; @schindler2017]. Loss Aversion has a strong connection to Negativity Bias Theorem since negative cues might be used as an early warning against potential losses [@herr1991].

Negativity Bias Theorem has four main aspects, namely, higher negative potency -- Negative signals have more impact on people than the positive ones. --, steeper negative gradients -- Negativity of negatives events grows faster in space and time than positive ones. --, higher negative dominance -- A combination of equally negative and positive cues results in more negative impressions. --, and various negative differentiations -- Negative cues produce more diverse individual responses than positive ones. --[@rozin2001; @sen2007; @eslami2018].

## Some other theory

## some other

# The Data and Research Methodology

## Data Collection

The data was scraped with a custom web scraper which was constructed with Selenium Framework for Pyhton 3 [@python3]. The scraper got the customer reviews from IMDB and BestBuy. The former is the biggest platform for Movies and TV Shows, the latter is one of the biggest electronic retailers in United States of America. Both websites host a great amount of online reviews. After the data was collected it was further transformed and analyzed with the help of R Programming Language [@RCore; @Tidyverse; @Tidytext].

Online Customer Reviews from IMDB were collected as an experience goods data since movies can be considered as pure-experience goods and their search attributes are very limited. It is nearly impossible to evaluate a movie before watching it. Furthermore, evaluations for a movie can both have subjective (personal attachment to the protagonist, atmosphere of the movie etc.) and objective aspects (writing, production quality etc.). There are also numerous studies that analyzed movies as experience goods[@nelson1970; @liu2014; @ghose2006; @baek2012; @lee2016].

BestBuy is a very big electronics retailer and this is why the website offers a sizable number of OCR for search goods. Contrary to experience goods, search goods usually have more objectively quantifiable traits. For example, resolution or screen size of a TV, RAM or GPU of a Laptop, storage or battery lifetime of a Smart Phone. Additionally, in the literature, TVs, Laptops and Phones were used as search goods [@mudambi2010; @sun2019; @baek2012; @ren2019]. Therefore, search goods data was collected from BestBuy.

-   mention that channels can change the way we perceive search and experience goods.

From IMDB reviews of 22 movies were collected as experience goods. In total, 22 movies account for 31,863 reviews. From BestBuy, total of 383,357 reviews were collected as search goods. Specifically, 201,113 reviews for TVs, 130,270 for Laptops and 51,974 for Phones. However, in the data there are many reviews that were never voted. To make sure that I am analyzing reviews that were actually read by people, I set a threshold for Total Reviews as five. In other words, reviews that had less than 5 Total Votes were removed from the analysis data [@choi2020; @baek2012]. At the very end, the analysis data was consisted of 15,024 observations for experience goods and 8,042 observations for search goods. It should be noted that `r round(15024 / 31863, digits = 2) * 100` percent of the reviews had total votes of 5 or greater for experience goods, whereas only `r round(8042 / 383357, digits = 2)* 100` percent of the search goods were voted at least 5 times. Most of the search product reviews aren't getting voted and even possible that they aren't getting read at all. This might support the idea that people form more homogeneous opinions about search goods [@hong2014]. Once a helpful review is written most of the other reviews become redundant since other reviews most likely conveys similar information. On the contrary, more attendance to reviews can be observed for experience goods (movies) since nearly half of them were voted at least five times. This might be because people form heterogeneous opinions about experience goods [@hong2014].

As a data cleaning note, in IMDB data there were cases (20 observations to be exact) where the same reviews were written by different user names. All duplicates along with original reviews were removed because the reviews had different helpfulness ratios.

## Exploratory Data Analysis

```{r Data_Prep_Experience}
# Experience Goods

## Read the Data

files <- dir()
files <- files[str_detect(files, "movie_.+")]

movies_data <- map_dfr(files, read_csv, show_col_types = FALSE)

## Removing reviews without Ratings
movies_data <- movies_data %>% 
  filter(!is.na(Rating)) 

## Creating Helpul and Total Votes Variables 
movies_data <- movies_data %>% 
  separate(Rating, into = c("Rating", "Total"), sep = "/", ) %>% 
  select(-Total) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Rating = as.double(Rating)) %>% 
  mutate(Helpfulness = Helpful_Vote / Total_Vote) %>% 
  mutate(Rating = Rating/2)

## Removing All the Duplicated Reviews including the originals since same reviews were written by differend user names
movies_data <- movies_data %>% 
  mutate(Rev_dup = duplicated(Review) | duplicated(Review, fromLast = TRUE)) %>% 
  filter(Rev_dup == 0)

# Words
## Creating Review ID
movies_data <- movies_data %>% 
  mutate(Doc_ID = row_number())

## Word Count
movies_agg <- movies_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

movies_data <- movies_data %>% 
  inner_join(movies_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- movies_data %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc") %>% 
               filter(!word %in% c("cage", "john","climax", "force", "king", "mad",
                                   "fury", "hardy","margin", "harry", "venom",
                                   "marvel", "stark", "iron", "vulture", "green",
                                   "goblin", "bale"))) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
movies_data <- movies_data %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = ~ifelse(is.na(.), 0, .) ))

movies_data <- movies_data %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

movies_data <- movies_data %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))

## Freshness (Timeline)
movies_data <- movies_data %>% 
  mutate(Date = dmy(Date, locale = "english"))

first_dates <- movies_data %>% 
  group_by(Movie) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Movie, Date) %>% 
  rename(First_Date = Date)

movies_data <- full_join(movies_data, first_dates, by = "Movie") %>% 
  mutate(Freshness = as.integer(Date - First_Date))

```

```{r Data_Prep_Search}

# Search Goods
## Phone Data
phones_data <- read_csv("all_phone_reviews_15_03_22.csv", show_col_types = FALSE)

phones_data <- phones_data %>% 
  mutate(Product_Type = "Phone")

## Laptop Data
files <- dir()
files <- files[str_detect(dir(), "all_laptop_reviews.+")]

laptop_data <- map_dfr(files, read_csv, show_col_types = FALSE)

### Removing replicates that I couldn't detect earlier
laptop_data <- laptop_data %>% 
  filter(!Product_Name %in% c(
    'Samsung - Galaxy Book Pro 360 15.6" AMOLED Touch-Screen Laptop - Intel Evo Platform Core i7 - 16GB Memory - 1TB SSD - Mystic Navy',
    'ASUS - 14.0" Laptop - Intel Celeron N4020 - 4GB Memory - 64GB eMMC - Star Black - Star Black',
    'ASUS - 14" Chromebook - Intel Celeron N3350 - 4GB Memory - 32GB eMMC - Silver',
    'Samsung - Galaxy 13.3" 4K Ultra HD Touch-Screen Chromebook - Intel Core i5 - 8GB Memory - 256GB SSD - Mercury Gray'
  )) %>% 
  mutate(Product_Type = "Laptop")

## TV Data
tvs_data <- read_csv("all_tv_reviews_16_03_22.csv", show_col_types = FALSE)
tvs_data <- tvs_data %>% 
  mutate(Product_Type = "TV")

## Merging All 
search_goods <- phones_data %>% 
  bind_rows(tvs_data, laptop_data)

## Creating Total Votes variable Filtering Total Votes below 5

search_goods <- search_goods %>% 
  mutate(Total_Vote = Helpful + Unhelpful) %>% 
  filter(Total_Vote > 4) %>% 
  mutate(Helpfulness = Helpful / Total_Vote) %>% 
  mutate(is_Exp = 0)

search_goods <- search_goods %>% 
  mutate(Dupped = duplicated(Review)) %>% 
  filter(Dupped == 0)

## Word Counts
### Creating Review IDs
search_goods <- search_goods %>% 
  mutate(Doc_ID = row_number()) 

words_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  count(Doc_ID, name = "Word_Count")

search_goods <- search_goods %>% 
  inner_join(words_agg, by = "Doc_ID")

## Emotion Sentiments (EmoLex)
### log10(Proportion of Emotion Words * 100) 

sentiments_agg <- search_goods %>%
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = word, input = Review, to_lower = FALSE) %>% 
  inner_join(get_sentiments("nrc")%>% 
               filter(!word %in% c("cage", "john","climax", "force", "king", "mad",
                                   "fury", "hardy","margin", "harry", "venom",
                                   "marvel", "stark", "iron", "vulture", "green",
                                   "goblin", "bale"))) %>% 
  count(Doc_ID, sentiment, name = "Sent_Count") %>% 
  pivot_wider(names_from = sentiment, values_from = Sent_Count, values_fill = 0)
  
search_goods <- search_goods %>% 
  full_join(sentiments_agg, by = "Doc_ID")

variables <- names(sentiments_agg)[-1]

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = ~ifelse(is.na(.), 0, .) ))


search_goods <- search_goods %>% 
  mutate(across(.cols = all_of(variables), .fns = function(.) ./Word_Count*100))

search_goods <- search_goods %>% 
  mutate(across(.cols = variables, .fns = function(.) log10(. + 1)))


## Freshness (Timeline)
search_goods <- search_goods %>% 
  mutate(Date = as_date(mdy_hm(Date, locale = "english")))

first_dates <- search_goods %>% 
  group_by(Product_Name) %>% 
  slice_min(order_by = Date, n = 1, with_ties = F) %>% 
  select(Product_Name, Date) %>% 
  rename(First_Date = Date)

search_goods <- search_goods %>% 
  full_join(first_dates, by = "Product_Name") %>% 
  mutate(Freshness = as.integer(Date - First_Date))
```

In this section both dependent and independent variables are going to be introduced. The goal of this section is not "confirmatory" whatsoever. As a researcher, I believe sharing the data that I used in the study transparently increases the research's reproducibility value.

In this section there will be univariate visualizations of all variables and bivariate visualizations of independent variables against the dependent variable.

-   elaborate more on the importance of EDA here

### Dependent Variable

$$
Perceived\ Helpfulness = \frac{Helpful\ Votes}{Total\ Votes}
$$

The ratio of helpfulness is derived from the formula above. Below on Figure \@ref(fig:IV-Hist-Search) the distribution of the helpfulness for search goods can be observed.

```{r IV-Hist-Search, fig.cap="Distribution of the Helpfulness of Search Goods"}

search_goods %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = NULL)
  
```

As it can be seen the distribution is rather skewed. Looks like in the electronics case, readers find some reviews very helpful. In fact, in the data there is one review which had over 170 helpful votes with zero unhelpful votes. On the other hand, distribution for experience goods on Figure \@ref(fig:IV-Hist-Exp) presents a different picture. This time helpfulness ratio is sort of normally distributed. There aren't very helpful reviews. The most helpful review interval seems to be lying between 70% to 80%.

```{r IV-Hist-Exp, fig.cap="Distribution of the Helpfulness of Experience Goods"}
movies_data %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 10)+
  labs(y = NULL)
```

@hong2014 states that consumers likely to form homogeneous opinions about search goods [@sun2019]. On the contrary, the same paper suggests that consumers' opinions are more heterogeneous on experience goods. Intuitively, what these histograms show is the same. For instance, positive or negative attributes of search goods can be expressed in a limited number of ways. The storage space of a phone might be small or CPU of a laptop might not be fast as expected. Thus, one good review is enough to reveal the quality of a product. I think this is why we see such a big spike at 100% on Figure \@ref(fig:IV-Hist-Search). In the movies (experience good) case on Figure \@ref(fig:IV-Hist-Exp), we see that readers usually disagree with each other because the distribution of the perceived helpfulness distributed around 50%. This might be because evaluation process of movies is rather subjective. One person might not like Spider-Man: Homecoming and another would love it. This might depend on how a person idealizes the Spider-Man or Peter Parker image in her mind which is completely subjective.

### Independent Variables

#### Review Rating (Review Extremity)

Review Rating is the star rating that was given by the reviewer as an evaluation of a product.

For both search and experience goods the bi-modality of the distributions can be observed on Figure \@ref(fig:Search-Rating) (A) and Figure \@ref(fig:Experience-Rating) (A) respectively. @hu2006 also pointed out a similar distribution concerning Online Product Reviews. This might mean that consumers usually tend to write product reviews when they are extremely happy or unhappy with the product.

Because the data was taken from two different websites (BestBuy and IMDB), there are two different scales. For IMDB that is 1 to 10 and for BestBuy the scale is between 1 and 5. To make the interpretation of the regression coefficients uniform, IMDB's scale was divided by 2.

On Figure \@ref(fig:Search-Rating) (B) and Figure \@ref(fig:Experience-Rating) (B) the bivariate relationship between perceived helpfulness and review extremity can be seen. Again, as a researcher, I must emphasize that this analysis is not confirmatory. On figures, we can observe "U" type of relationship for both Search and Experience goods, though the non-linearity seems to be stronger with Search goods. This might suggest that on the average extreme reviews might be more helpful than moderate reviews [@mudambi2010; @cao2011; @liu2014; @wang2018; @ghose2006]. On the other hand, it is observable that positive reviews on the average are more helpful than the rest. This might indicate that there is a linear positive relationship [@pan2011; @schindler2012; @korfiatis2012; @lee2016; @choi2020].

```{r Search-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Search Goods."}
p1 <- search_goods %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  labs(y = "Count", x = "Review Rating")

p2 <- search_goods %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(method = "loess", color = "steelblue")+
  labs(x = "Review Rating")

plots <-  p1 + p2

plots + plot_annotation(tag_levels = "A")
```

```{r Experience-Rating, fig.cap="The Distribution(A) and the Scatter Plot(B) of Review Rating for Experience Goods."}
p1 <- movies_data %>% 
  ggplot(aes(Rating))+
  geom_bar(fill = "steelblue")+
  scale_x_continuous(n.breaks = 10)+
  labs(y = "Count", x = "Review Rating")

p2 <- movies_data %>% 
  ggplot(aes(Rating, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 10)+
  labs(x = "Review Rating")

plots <-  p1 + p2

plots + plot_annotation(tag_levels = "A")
```

#### Review Length (Review Depth)

Review Length is the number of the words in a given review.

The exponential distribution of the number of words can be observed for both good types on Figure \@ref(fig:Search-Length) (A) and Figure \@ref(fig:Experience-Length) (A). This means that reviews become less prevalent as they get longer. This might suggest logarithmic transformation can increase the model fit. For the transformed distributions see Figure \@ref(fig:Search-Length) (B) and Figure \@ref(fig:Experience-Length) (B).

The bivariate relationship between the number of words and Helpfulness seems to be varying for different product types. For Search goods on Figure \@ref(fig:Search-Length) (C) there is a clear positive relationship [@mudambi2010; @hong2017; @liu2014; @wang2018; @korfiatis2012; @lee2016; @lee2014; @choi2020]. However, this positive relationship appears to stop at a threshold [@schindler2012; @huang2015; @baek2012; @kuan2015; @eslami2018]. With this in mind, logarithmic transformation might make more sense (see Figure \@ref(fig:Search-Length) (D)) for modelling purposes. For Experience goods, on the other hand, there seems to be a weak negative relationship or no relationship at all (see Figure \@ref(fig:Experience-Length) (C) and (D)).

```{r Search-Length, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Number of the Words")

p1_log <- search_goods %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Number of the Words)")

p2 <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Number of the Words")

p2_log <- search_goods %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Number of the Words)")

plots <-  (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Length, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Length for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Number of the Words")

p1_log <- movies_data %>% 
  ggplot(aes(Word_Count))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Number of the Words)")

p2 <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Number of the Words")

p2_log <- movies_data %>% 
  ggplot(aes(Word_Count, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Number of the Words)")

plots <-  (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Freshness of A Review

Review Freshness indicates how recent a review is. To engineer this variable following steps were taken:

1.  The first review date for a particular brand and model of a product (Movie, Phone, Laptop, TV) was taken.
2.  The first dates for particular products then subtracted from other reviews' dates under the respective brand and model resulting an integer variable representing days.
3.  Bigger numbers represent more recent reviews.

Distributions of both product types resemble exponential distribution (see Figure \@ref(fig:Search-Freshness) (A) and Figure \@ref(fig:Experience-Freshness) (A)). Again a logarithmic transformation might be useful here (see Figure \@ref(fig:Search-Freshness) (B) and Figure \@ref(fig:Experience-Freshness) (B)) . Interestingly, even after 3 years from the release date some products are still getting reviewed from customers.

On the scatter plots a weak relationship between perceived helpfulness and review freshness could be observed. The smoother line suggests both positive [@lee2014; @choi2020; @wu2021] and negative [@liu2014; @li2019; @zhou2019] linear relationship between two variables. This non-linearity may explain different findings in the literature.

```{r Search-Freshness, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Search Goods and respective logarithmic transformations (B), (D)"}
p1 <- search_goods %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Freshness of Review")

p1_log <- search_goods %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Freshness of Review + 1)")

p2 <- search_goods %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Freshness of Review")

p2_log <- search_goods %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

```{r Experience-Freshness, fig.height=6, fig.cap="The Distribution(A) and the Scatter Plot(C) of Review Freshness for Experience Goods and respective logarithmic transformations (B), (D)"}
p1 <- movies_data %>% 
  ggplot(aes(Freshness))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = "Freshness of Review")

p1_log <- movies_data %>% 
  ggplot(aes(Freshness + 1))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 10)+
  scale_x_log10(n.break = 8)+
  labs(y = NULL, x = "log10(Freshness of Review + 1)")

p2 <- movies_data %>% 
  ggplot(aes(Freshness, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(x = "Freshness of Review")

p2_log <- movies_data %>% 
  ggplot(aes(Freshness + 1, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_log10(n.breaks = 8)+
  labs(x = "log10(Freshness of Review + 1)")

plots <- (p1 + p1_log) / (p2 + p2_log)

plots + plot_annotation(
  tag_levels = "A"
)

```

#### Proportion of Negative Words

Negative words were derived by using NRC Word-Emotion Association Lexicon [@Mohammad13]. A lexicon is basically a dictionary of words. These words might have similar emotions or sentiments. In this case, negative words lexicon was used in order to engineer this variable. A sample of negative words from the lexicon can be observed below.

> *"caricature, superstition, compost, misconception, grief, violation, scrub, omen, erratic, porcupine"*

$$
Negative\ Word \% = \frac{Number\ of\ Matched\ Negative\ Word}{Total\ Words\ in\ the\ Review} 
$$

These steps were taken in order to create this variable:

1.  Words in a review were tokenized.
2.  Tokenized words were matched with the words in the lexicon.
3.  Matches were counted and divided by the total number of words with the respective review then multiplied by 100 in order to make regression results more interpretable.
4.  Logarithmic transformation was applied in order to scale the variable in a more normalized way.

On Figure \@ref(fig:Search-Neg) (A) and Figure \@ref(fig:Experience-Neg) (A) the distributions of the variable for both good types can be seen. On plots, x-axis labels were given in the logarithm of base 10 so 1.00 on the x-axis implies that 10% of the review was negative words. It should be noted that `r round(mean(search_goods$negative == 0), digits = 2)` percent of the reviews for search goods don't have any negative words in them. For experience goods this number is `r round(mean(movies_data$negative == 0), digits = 2)` percent.

On Figure \@ref(fig:Search-Neg) (B), non-linear relationship between perceived helpfulness and the proportion of negative words can be observed for search goods. This might suggest that too much negative information might fall into slander rather than informing the customer about the potential risks of the product. On the other hand, somewhat weak negative linear relationship between perceived helpfulness and proportion of negative words can be seen for experience goods on Figure \@ref(fig:Experience-Neg) (B).

```{r Search-Neg, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Search Goods"}
p1 <- search_goods %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Negative Words)")

p2 <- search_goods %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Neg, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Negative Words for Experience Goods"}
p1 <- movies_data %>% 
  ggplot(aes(negative))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Count", x = "Log10(Proportion of Negative Words)")

p2 <- movies_data %>% 
  ggplot(aes(negative, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Negative Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

#### Proportion of Positive Words

$$
Positive\ Word \% = \frac{Number\ of\ Matched\ Positive\ Word}{Total\ Words\ in\ the\ Review} 
$$

This variable was engineered in the same manner with its negative words counterpart. In this case, however, reviews of both good types have more positive words in them proportionally when compared to negative words. `r round(mean(search_goods$positive == 0), digits = 2)` percent of the reviews of search goods don't have any positive words in them and this number is only `r round(mean(movies_data$positive == 0), digits = 2)` for experience goods. Except for zero values both distributions are looking quite normal (see Figure \@ref(fig:Search-Pos) (A) and Figure \@ref(fig:Experience-Pos) (A)).

*A sample of positive words from EmoLex:*

> *"covenant, jovial, score, foundation, affirmation, reliable, mending, garden, luxurious, wit"*

On scatter plots (see Figure \@ref(fig:Search-Pos) (B) and Figure \@ref(fig:Experience-Pos) (B)), relationships between perceived helpfulness and proportion of positive words are looking very similar to their negative counterparts. Furthermore, these plots kind of similar to perceived helpfulness versus number of words plots (see Figure \@ref(fig:Search-Length) (B) and Figure \@ref(fig:Experience-Length) (B)). In a sense, both negative and positive proportions imply number of words. As the proportion increases so does the number of words. That's why aforementioned figures show similar relationships between variables. On higher dimensions these relationships might be different. That is why Exploratory Data Analysis should never be conducted in a confirmatory sense.

```{r Search-Pos, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Positive Words for Search Goods"}
p1 <- search_goods %>% 
  ggplot(aes(positive))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = NULL)

p2 <- search_goods %>% 
  ggplot(aes(positive, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Positive Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

```{r Experience-Pos, fig.cap="The Distribution(A) and the Scatter Plot(B) of Proportion of Positive Words for Experience Goods"}
p1 <- movies_data %>% 
  ggplot(aes(positive))+
  geom_histogram(color = "white", fill = "steelblue", boundary = 0)+
  scale_x_continuous(n.breaks = 8)+
  labs(y = NULL, x = NULL)

p2 <- movies_data %>% 
  ggplot(aes(positive, Helpfulness))+
  geom_point(alpha = 0.1)+
  geom_smooth(color = "steelblue")+
  scale_x_continuous(n.breaks = 8)+
  labs(y = "Helpfulness", x = "Log10(Proportion of Positive Words)")

plots <- p1 + p2

plots + plot_annotation(
  tag_levels = "A"
)
```

### Control Variables

"Total Votes" is going to be used as a control variable since 80% Perceived Helpfulness can arise from both 4 over 5 Helpful votes or 80 over 100 Helpful votes [@mudambi2010; @choi2020].

### Correlation Table

Before progressing to the modelling section, it would be a good practice to check the correlations between independent and control variables in order not to cause any multi-colinearity problems. As it can be seen on Figure \@ref(fig:Correlations) there aren't any major correlations between independent and control variables for both good types.

The relatively high positive/negative correlation between the Review Rating and Positive/Negative word proportions might give a hint about the usefulness of the lexicon. As the Review Rating gets higher the review contains more positive words and, on the other hand, low rated reviews have more negative words in them. This makes sense intuitively.

```{r Correlations, fig.cap="Correlations between Independent Variables.", fig.height=8}



p1 <- search_goods %>% 
  select("Review Rating" = Rating,
         "Review Length" = Word_Count, Freshness,
        "Positive Proportion" = positive,
        "Negative Proportion" = negative,
        "Total Votes" = Total_Vote) %>% 
  mutate(`Review Length` = log10(`Review Length`),
         `Total Votes` = log10(`Total Votes`)) %>% 
  ggcorr(label = TRUE, hjust = 0.75, nbreaks = 5,low = "steelblue", mid = "grey95",
         high = "darkred", label_round = 2, layout.exp = 1)+
  labs(title = "Search Goods")+
  theme_void()


p2 <- movies_data %>% 
  select("Review Rating" = Rating,
         "Review Length" = Word_Count, Freshness,
        "Positive Proportion" = positive,
        "Negative Proportion" = negative,
        "Total Votes" = Total_Vote) %>% 
  mutate(`Review Length` = log10(`Review Length`),
         `Total Votes` = log10(`Total Votes`)) %>% 
  ggcorr(label = TRUE, hjust = 0.75, nbreaks = 5,low = "steelblue", mid = "grey95",
         high = "darkred", label_round = 2, layout.exp = 1)+
  labs(title = "Experience Goods")+
  theme_void()

(p1 / p2) +
  plot_layout(guides = "collect")
```

## Hypothesis Development

@korfiatis2012 found that neutral rated reviews (3 out of 5 stars) imply indifference to readers. This means that prior information of the potential buyer can't be influenced by these neutral reviews and likely to be found less helpful compared to extreme reviews (1 or 5 stars) [@wang2018; @liu2014; @cao2011; @ghose2006]. As the reviews get more extremely rated they might strengthen or sway prior information that a potential consumer has, making them more helpful. This extremeness has more objective nature for Search Goods since there are fewer ways to express pleasantness or unpleasantness for a particular product. For instance a bad design in a laptop might cause fans to make much more noise than they should normally do. This information along with a very low rating may signal there are some inherent problems with that particular product, making the feedback more helpful. On the other hand, for Experience Goods and in this specific case movies, opinions for a given product might be much more subjective. For example, a reviewer might find a movie "childish" and give it one star, the other might find the movie "light-hearted and colorful" and give 5 stars, making the helpfulness subjective and decreasing its usefulness. Furthermore, for experience goods, it was found that opposite extremes to a potential buyer's prior expectations aren't impactful on the decision making process [@hoch1989]. In other words, prior expectations play a bigger role in decision making process for experience goods. Therefore it is hypothesized that:

***H1**: There is a U-Shaped relationship between Perceived Helpfulness and Review Rating. This U-Shape is stronger/weaker for Search/Experience Goods.*

In order to decrease the uncertainty, search costs must be reduced. In \@ref(search-and-experience-goods-classification) it was mentioned that the action of search was costlier for Experience Goods than Search Goods. Furthermore, people tend to have homogeneous opinions about Search Goods [@hong2014]. This might mean that information cues are much more uniform for Search Goods. [@huang2015; @baek2012]. In other words, there are limited ways to construct a review because search attributes are more objective. Therefore, there might be a positive linear relationship between Perceived Helpfulness and Review Length. However, since the attributes of electronics are objectively identifiable and limited (storage space of a phone, RAM of a laptop, color depth of a TV etc.) there aren't much to review for a reviewer. Because of this, there might be diminishing marginal returns to Review Length [@schindler2012; @huang2015]. On the contrary, for Experience Goods, people have tendency to form heterogeneous opinions [@hong2014]. This means that movie reviews are much more subjective by nature. Since there are less objectivity, a helpful review to one person might not be helpful to another. However, since there are other ways to objectively evaluate a movie, a weaker positive linear relationship is expected. Therefore it is hypothesized that:

***H2**: There is a positive relationship between logarithm of Review Length and Perceived Helpfulness. This effect is stronger for Search Goods than Experience Goods*.

As it was mentioned in \@ref(negativity-bias-theorem-and-loss-aversion), people tend to try to minimize their losses. Because of this, negative cues draw more attention than positive ones since negative cues may imply potential losses. @guo2019 found out that people spent more time when evaluating negative reviews. This may suggest a linear interaction between Review Extremity and Review Length. Since people take more time to evaluate negative reviews, this may indicate that the effect of Review Length is stronger when Review Rating is low. And when a review is longer it is more helpful for negatively rated reviews than positively rated reviews. In this case, it is hypothesized that:

***H3**: There is a negative linear interaction effect between Review Rating and the logarithm of the Review Length for both Search and Experience Goods.*

Potential buyers may find more recent reviews helpful because they may want to get updated information on a product. This might especially be true for products which are on the market for more than a year. For instance, a laptop which was produced in 2018 could be a good choice for that year but for year 2021 it is probable that its features and specifications would be below average. A similar behavior might be possible for movies as well. However, this time reviewers might need to take their time to digest and evaluate the movie. A review which is carefully constructed may be posted later making it more recent compared to a review which was hastily written. Furthermore, some popular movies which are considered as classics may attract reviews even way later than they were first released. Since some websites may employ algorithms that may push newer reviews to the top in order to be able to increase the interaction on their platforms, these kind of reviews may attract more helpful votes. Thus, it is hypothesized that:

***H4**: There is a positive relationship between Perceived Helpfulness and Freshness of a Review for both Search and Experience Goods.*

So far it should be understood that people try to cut down their losses. That is why negative cues draw more attention than positive ones [@kahneman1979; @schindler2017]. Additionally, people find negative information more credible and persuasive than positive information [@ito1998; @Kanouse1984ExplainingNB]. It is known that Review Rating is the satisfaction level of a customer towards a product. However, Review Rating itself isn't enough to convey the information about how a customer expresses her satisfaction / dissatisfaction towards a product. The way a customer expresses her opinion on a product might have an effect on the perceived helpfulness of a review. Reviews with negative sentiments may bring out the risks associated with the purchase better than reviews with positive sentiments. Since in this study movies and electronics are compared, it is costlier to make a wrong decision on Search Goods than Experience Goods. Therefore, it is hypothesized that:

***H5**: There is a positive relationship between the Proportion of Negative Words and Perceived Helpfulness. This effect is stronger / weaker for Search/Experience goods.*\
***H6**: There is a negative relationship between the Proportion of Positive Words and Perceived Helpfulness for both product types.*

## Research Methodology

Tobit model is also named as Censored Regression model. Censored data arises when there exist lower (censored from below) or upper (censored from above) or both lower and upper boundaries for the dependent variable. This type of data is fairly prevalent in Economics. For instance, in consumer economics there is no such thing as "negative consumption" or in labor economics there is no such thing as "negative work hours". Furthermore, there might be cases where some really high values could be recorded as "50,000 or more" because of the data collection design. Ordinary Least Squares can't take account of the censored observations in the data because it extrapolates lower and upper ends when it fits the model and this leads biased estimations.

Tobit regression approaches censored dependent variable problem from two angles. In a sense Tobit regression could be considered as the summation of Probit regression and Truncated regression. The probit side considers if a particular observation is above or below the given limit. If the observation, for example, is below the lower limit then it must be equal to the lower limit. If it is higher than the lower limit the problem turns to a normal regression. In this case, Perceived Helpfulness can be expressed such as this:

$$
y=
\begin{cases}
  0   & \text{if  $ \ y^* \le0$} \\
  y^* & \text{if $ \ 0< y^* <1$} \\
  1   & \text{if $ \ y^* \ge 1$}
\end{cases}
$$

Tobit Regression is selected as the analysis method because of three reasons. First of all, the dependent variable, perceived helpfulness, is defined in a continuous range from 0 to 1 since perceived helpfulness is a percentage as mentioned above. The second reason as stated by @mudambi2010 is the potential selection bias in the sample. Unfortunately, it is not possible to know how many people read a particular review. Neither BestBuy nor IMDB disclose this information. Because of this, being included in a sample could be correlated with independent variables. For instance, potentially more voted reviews could attract more readers or readers could be drawn to reviews which have certain ratings. Lastly, there are many studies from the literature that used Tobit Regression as their research method [@mudambi2010; @korfiatis2012; @huang2015; @choi2020; @ren2019].

# Results

Tobit regression results are presented at Table \@ref(tab:Reg-Results) alongside with OLS results as a benchmark [@mudambi2010; @kuan2015; @choi2020].

Hyphothesis 1 proposed a U-Shape for the relationship between Review Extremity and Perceived Helpfulness. For Search Goods this is true because the linear Rating Extremity coefficient (-0.061, p \< 0.01) has a negative sign and the quadratic Rating Extremity coefficient (0.036, p \< 0.001) has a positive sign. The presence of the significant linear interaction effect between Review Extremity and Review Length accentuates the linear Rating Extremity coefficient (-0.043, p \< 0.001). This means that as reviews get longer negative extreme reviews become more useful. For Experience Goods, on the other hand, both linear (0.026, p \< 0.001) and quadratic (0.006, p \< 0.001) Review Extremity terms are positive. A U-Shaped function can't be observed. However, the significant interaction effect between Review Length and Review Extremity (-0.015, p \< 0.001) suggests that for longer reviews the linear term turns to negative. This means that the function takes a U-Shape after a certain review length. It can also be observed that Search Goods have stronger coefficients than Experience Goods meaning the expected U-Shape is more prominent for Search Goods. Therefore, it can be said that for shorter reviews Hypothesis 1 is partially supported and for longer reviews it is fully supported.

A positive linear relationship between the logarithm of Review Length and Perceived Helpfulness was proposed with Hypothesis 2. A strong relationship between Review Length and Perceived Helpfulness is observed (0.404, p \< 0.001) for Search Goods. This effect is moderated by Review Extremity since the interaction effect is significant with a negative sign (-0.043, p \< 0.001). As review rating increases the responsiveness of the Perceived Helpfulness to Review Length decreases. This means that for positive extreme reviews relatively shorter texts are more helpful. Logarithmic form also proves that words in a review shows diminishing returns to helpfulness. Experience Goods somewhat deviates from the results of Search Goods. There is a weak positive relationship between Review Length and Perceived Helpfulness (0.021, p \< 0.01). However, this positive relationship quickly turns to negative for higher ratings because of the presence of the interaction term between Review Length and Review Extremity (-0.015, p \< 0.001). For lower ratings Hypothesis 2 is fully supported but as the Review Extremity increases this support weakens.

For both good types the interaction effects are significantly negative (-0.043, p \< 0.001 ; -0.015, p \< 0.001). This means that as Review Extremity increases longer reviews become less helpful, supporting the idea of people spending more time evaluating negative information because it takes more time to read a longer review. However, there are nuances too. For Search Goods, the effect of the Review Length is always positive whether or not Review Extremity is low or high. On the contrary, for Experience Goods, after some point of Review Extremity longer reviews become "unhelpful" because the sign of the Review Length turns to negative. In this case Hypothesis 3 is supported wholly for Search Goods and somewhat supported for Experience Goods.

Hypothesis 4 is fully supported. Recent reviews seem to be more helpful for both good types (0.029, p \< 0.001 ; 0.038, p \< 0.001).

The results for Negative Word Proportion is quite interesting because it shows different characteristics for different product types. As expected, for Search Goods, Negative Word Proportion has a significant positive coefficient (0.061, p \< 0.001). The presence of negative words indeed increases the perceived helpfulness. Contrary to this, there is a negative relationship between negative words and perceived helpfulness for Experience Goods (-0.021, p \< 0.001). Hypothesis 5 is only supported for Search Goods and not supported for Experience Goods.

Positive Word Proportion is only significant for Search Goods (-0.056, p \< 0.001). The presence of positive words has a negative effect on perceived helpfulness just as expected. The coefficient for Experience Goods show the expected sign too but it is statistically insignificant (-0.007, p \> 0.5). With this result, Hypothesis 6 is again only supported for Search Goods and not supported for Experience Goods.

The result of the only control variable should also be reported. It seems that total amount of votes that a review gets has a negative relationship (-0.157, p \< 0.001) with perceived helpfulness for Search Goods. The opposite is true for Experience Goods (0.123, p \< 0.001).

```{r}
search_tobit <- tobit(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = search_goods,
                     left = 0, right = 1)

experience_tobit <- tobit(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = movies_data,
                     left = 0, right = 1)


search_lm <- lm(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = search_goods)

experience_lm <- lm(Helpfulness ~ Rating + I(Rating^2) + log10(Word_Count) + log10(Total_Vote) + log10(Word_Count) * Rating + log10(Freshness+1) + positive + negative,
                     data = movies_data)

```

```{r Reg-Results, results='asis'}
stargazer(list("Search Tobit" = search_tobit,
               "Experience Tobit" = experience_tobit,
               "Search Linear" = search_lm,
               "Experience Linear" = experience_lm),
          type = "latex",
          header = FALSE,
          model.numbers = FALSE,
          column.labels = c("Search", "Experience", "Search", "Experience"),
          covariate.labels = c("Rating", "Rating Squared", "log10(Review Length)",
                               "log10(Total Vote)", "log10(Freshness + 1)",
                               "Positive Proportion",  
                               "Negative Proportion", "Rating x Review Length"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          label = "tab:Reg-Results")
```

# Discussion

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

# Conclusion

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

\*\*WORK IN PROGRESS\*\*

# References
